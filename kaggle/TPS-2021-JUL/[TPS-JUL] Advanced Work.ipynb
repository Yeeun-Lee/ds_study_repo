{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "organized-limitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.2.1-py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\yeeunlee\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from lightgbm) (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\yeeunlee\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from lightgbm) (1.19.5)\n",
      "Requirement already satisfied: wheel in c:\\users\\yeeunlee\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\yeeunlee\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from lightgbm) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\yeeunlee\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yeeunlee\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "median-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interim-shape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package lightgbm:\n",
      "\n",
      "NAME\n",
      "    lightgbm - LightGBM, Light Gradient Boosting Machine.\n",
      "\n",
      "DESCRIPTION\n",
      "    Contributors: https://github.com/microsoft/LightGBM/graphs/contributors.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    basic\n",
      "    callback\n",
      "    compat\n",
      "    dask\n",
      "    engine\n",
      "    libpath\n",
      "    plotting\n",
      "    sklearn\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        lightgbm.basic.Booster\n",
      "        lightgbm.basic.Dataset\n",
      "        lightgbm.engine.CVBooster\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        lightgbm.sklearn.LGBMModel\n",
      "            lightgbm.sklearn.LGBMClassifier(lightgbm.sklearn.LGBMModel, sklearn.base.ClassifierMixin)\n",
      "                lightgbm.dask.DaskLGBMClassifier(lightgbm.sklearn.LGBMClassifier, lightgbm.dask._DaskLGBMModel)\n",
      "            lightgbm.sklearn.LGBMRanker\n",
      "                lightgbm.dask.DaskLGBMRanker(lightgbm.sklearn.LGBMRanker, lightgbm.dask._DaskLGBMModel)\n",
      "            lightgbm.sklearn.LGBMRegressor(lightgbm.sklearn.LGBMModel, sklearn.base.RegressorMixin)\n",
      "                lightgbm.dask.DaskLGBMRegressor(lightgbm.sklearn.LGBMRegressor, lightgbm.dask._DaskLGBMModel)\n",
      "    \n",
      "    class Booster(builtins.object)\n",
      "     |  Booster(params=None, train_set=None, model_file=None, model_str=None, silent=False)\n",
      "     |  \n",
      "     |  Booster in LightGBM.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(self)\n",
      "     |  \n",
      "     |  __deepcopy__(self, _)\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, params=None, train_set=None, model_file=None, model_str=None, silent=False)\n",
      "     |      Initialize the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params : dict or None, optional (default=None)\n",
      "     |          Parameters for Booster.\n",
      "     |      train_set : Dataset or None, optional (default=None)\n",
      "     |          Training dataset.\n",
      "     |      model_file : string or None, optional (default=None)\n",
      "     |          Path to the model file.\n",
      "     |      model_str : string or None, optional (default=None)\n",
      "     |          Model will be loaded from this string.\n",
      "     |      silent : bool, optional (default=False)\n",
      "     |          Whether to print messages during construction.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_valid(self, data, name)\n",
      "     |      Add validation data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : Dataset\n",
      "     |          Validation data.\n",
      "     |      name : string\n",
      "     |          Name of validation data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Booster with set validation data.\n",
      "     |  \n",
      "     |  attr(self, key)\n",
      "     |      Get attribute string from the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : string\n",
      "     |          The name of the attribute.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : string or None\n",
      "     |          The attribute value.\n",
      "     |          Returns None if attribute does not exist.\n",
      "     |  \n",
      "     |  current_iteration(self)\n",
      "     |      Get the index of the current iteration.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      cur_iter : int\n",
      "     |          The index of the current iteration.\n",
      "     |  \n",
      "     |  dump_model(self, num_iteration=None, start_iteration=0, importance_type='split')\n",
      "     |      Dump Booster to JSON format.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Index of the iteration that should be dumped.\n",
      "     |          If None, if the best iteration exists, it is dumped; otherwise, all iterations are dumped.\n",
      "     |          If <= 0, all iterations are dumped.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration that should be dumped.\n",
      "     |      importance_type : string, optional (default=\"split\")\n",
      "     |          What type of feature importance should be dumped.\n",
      "     |          If \"split\", result contains numbers of times the feature is used in a model.\n",
      "     |          If \"gain\", result contains total gains of splits which use the feature.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      json_repr : dict\n",
      "     |          JSON format of Booster.\n",
      "     |  \n",
      "     |  eval(self, data, name, feval=None)\n",
      "     |      Evaluate for data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : Dataset\n",
      "     |          Data for the evaluating.\n",
      "     |      name : string\n",
      "     |          Name of the data.\n",
      "     |      feval : callable or None, optional (default=None)\n",
      "     |          Customized evaluation function.\n",
      "     |          Should accept two parameters: preds, eval_data,\n",
      "     |          and return (eval_name, eval_result, is_higher_better) or list of such tuples.\n",
      "     |      \n",
      "     |              preds : list or numpy 1-D array\n",
      "     |                  The predicted values.\n",
      "     |              eval_data : Dataset\n",
      "     |                  The evaluation dataset.\n",
      "     |              eval_name : string\n",
      "     |                  The name of evaluation function (without whitespaces).\n",
      "     |              eval_result : float\n",
      "     |                  The eval result.\n",
      "     |              is_higher_better : bool\n",
      "     |                  Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      "     |      \n",
      "     |          For binary task, the preds is probability of positive class (or margin in case of specified ``fobj``).\n",
      "     |          For multi-class task, the preds is group by class_id first, then group by row_id.\n",
      "     |          If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i].\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : list\n",
      "     |          List with evaluation results.\n",
      "     |  \n",
      "     |  eval_train(self, feval=None)\n",
      "     |      Evaluate for training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      feval : callable or None, optional (default=None)\n",
      "     |          Customized evaluation function.\n",
      "     |          Should accept two parameters: preds, train_data,\n",
      "     |          and return (eval_name, eval_result, is_higher_better) or list of such tuples.\n",
      "     |      \n",
      "     |              preds : list or numpy 1-D array\n",
      "     |                  The predicted values.\n",
      "     |              train_data : Dataset\n",
      "     |                  The training dataset.\n",
      "     |              eval_name : string\n",
      "     |                  The name of evaluation function (without whitespaces).\n",
      "     |              eval_result : float\n",
      "     |                  The eval result.\n",
      "     |              is_higher_better : bool\n",
      "     |                  Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      "     |      \n",
      "     |          For binary task, the preds is probability of positive class (or margin in case of specified ``fobj``).\n",
      "     |          For multi-class task, the preds is group by class_id first, then group by row_id.\n",
      "     |          If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i].\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : list\n",
      "     |          List with evaluation results.\n",
      "     |  \n",
      "     |  eval_valid(self, feval=None)\n",
      "     |      Evaluate for validation data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      feval : callable or None, optional (default=None)\n",
      "     |          Customized evaluation function.\n",
      "     |          Should accept two parameters: preds, valid_data,\n",
      "     |          and return (eval_name, eval_result, is_higher_better) or list of such tuples.\n",
      "     |      \n",
      "     |              preds : list or numpy 1-D array\n",
      "     |                  The predicted values.\n",
      "     |              valid_data : Dataset\n",
      "     |                  The validation dataset.\n",
      "     |              eval_name : string\n",
      "     |                  The name of evaluation function (without whitespaces).\n",
      "     |              eval_result : float\n",
      "     |                  The eval result.\n",
      "     |              is_higher_better : bool\n",
      "     |                  Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      "     |      \n",
      "     |          For binary task, the preds is probability of positive class (or margin in case of specified ``fobj``).\n",
      "     |          For multi-class task, the preds is group by class_id first, then group by row_id.\n",
      "     |          If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i].\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : list\n",
      "     |          List with evaluation results.\n",
      "     |  \n",
      "     |  feature_importance(self, importance_type='split', iteration=None)\n",
      "     |      Get feature importances.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      importance_type : string, optional (default=\"split\")\n",
      "     |          How the importance is calculated.\n",
      "     |          If \"split\", result contains numbers of times the feature is used in a model.\n",
      "     |          If \"gain\", result contains total gains of splits which use the feature.\n",
      "     |      iteration : int or None, optional (default=None)\n",
      "     |          Limit number of iterations in the feature importance calculation.\n",
      "     |          If None, if the best iteration exists, it is used; otherwise, all trees are used.\n",
      "     |          If <= 0, all trees are used (no limits).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : numpy array\n",
      "     |          Array with feature importances.\n",
      "     |  \n",
      "     |  feature_name(self)\n",
      "     |      Get names of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : list\n",
      "     |          List with names of features.\n",
      "     |  \n",
      "     |  free_dataset(self)\n",
      "     |      Free Booster's Datasets.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Booster without Datasets.\n",
      "     |  \n",
      "     |  free_network(self)\n",
      "     |      Free Booster's network.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Booster with freed network.\n",
      "     |  \n",
      "     |  get_leaf_output(self, tree_id, leaf_id)\n",
      "     |      Get the output of a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tree_id : int\n",
      "     |          The index of the tree.\n",
      "     |      leaf_id : int\n",
      "     |          The index of the leaf in the tree.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : float\n",
      "     |          The output of the leaf.\n",
      "     |  \n",
      "     |  get_split_value_histogram(self, feature, bins=None, xgboost_style=False)\n",
      "     |      Get split value histogram for the specified feature.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      feature : int or string\n",
      "     |          The feature name or index the histogram is calculated for.\n",
      "     |          If int, interpreted as index.\n",
      "     |          If string, interpreted as name.\n",
      "     |      \n",
      "     |          .. warning::\n",
      "     |      \n",
      "     |              Categorical features are not supported.\n",
      "     |      \n",
      "     |      bins : int, string or None, optional (default=None)\n",
      "     |          The maximum number of bins.\n",
      "     |          If None, or int and > number of unique split values and ``xgboost_style=True``,\n",
      "     |          the number of bins equals number of unique split values.\n",
      "     |          If string, it should be one from the list of the supported values by ``numpy.histogram()`` function.\n",
      "     |      xgboost_style : bool, optional (default=False)\n",
      "     |          Whether the returned result should be in the same form as it is in XGBoost.\n",
      "     |          If False, the returned value is tuple of 2 numpy arrays as it is in ``numpy.histogram()`` function.\n",
      "     |          If True, the returned value is matrix, in which the first column is the right edges of non-empty bins\n",
      "     |          and the second one is the histogram values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result_tuple : tuple of 2 numpy arrays\n",
      "     |          If ``xgboost_style=False``, the values of the histogram of used splitting values for the specified feature\n",
      "     |          and the bin edges.\n",
      "     |      result_array_like : numpy array or pandas DataFrame (if pandas is installed)\n",
      "     |          If ``xgboost_style=True``, the histogram of used splitting values for the specified feature.\n",
      "     |  \n",
      "     |  lower_bound(self)\n",
      "     |      Get lower bound value of a model.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      lower_bound : double\n",
      "     |          Lower bound value of the model.\n",
      "     |  \n",
      "     |  model_from_string(self, model_str, verbose=True)\n",
      "     |      Load Booster from a string.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      model_str : string\n",
      "     |          Model will be loaded from this string.\n",
      "     |      verbose : bool, optional (default=True)\n",
      "     |          Whether to print messages while loading model.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Loaded Booster object.\n",
      "     |  \n",
      "     |  model_to_string(self, num_iteration=None, start_iteration=0, importance_type='split')\n",
      "     |      Save Booster to string.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Index of the iteration that should be saved.\n",
      "     |          If None, if the best iteration exists, it is saved; otherwise, all iterations are saved.\n",
      "     |          If <= 0, all iterations are saved.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration that should be saved.\n",
      "     |      importance_type : string, optional (default=\"split\")\n",
      "     |          What type of feature importance should be saved.\n",
      "     |          If \"split\", result contains numbers of times the feature is used in a model.\n",
      "     |          If \"gain\", result contains total gains of splits which use the feature.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str_repr : string\n",
      "     |          String representation of Booster.\n",
      "     |  \n",
      "     |  num_feature(self)\n",
      "     |      Get number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      num_feature : int\n",
      "     |          The number of features.\n",
      "     |  \n",
      "     |  num_model_per_iteration(self)\n",
      "     |      Get number of models per iteration.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      model_per_iter : int\n",
      "     |          The number of models per iteration.\n",
      "     |  \n",
      "     |  num_trees(self)\n",
      "     |      Get number of weak sub-models.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      num_trees : int\n",
      "     |          The number of weak sub-models.\n",
      "     |  \n",
      "     |  predict(self, data, start_iteration=0, num_iteration=None, raw_score=False, pred_leaf=False, pred_contrib=False, data_has_header=False, is_reshape=True, **kwargs)\n",
      "     |      Make a prediction.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : string, numpy array, pandas DataFrame, H2O DataTable's Frame or scipy.sparse\n",
      "     |          Data source for prediction.\n",
      "     |          If string, it represents the path to txt file.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration to predict.\n",
      "     |          If <= 0, starts from the first iteration.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Total number of iterations used in the prediction.\n",
      "     |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      "     |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |      raw_score : bool, optional (default=False)\n",
      "     |          Whether to predict raw scores.\n",
      "     |      pred_leaf : bool, optional (default=False)\n",
      "     |          Whether to predict leaf index.\n",
      "     |      pred_contrib : bool, optional (default=False)\n",
      "     |          Whether to predict feature contributions.\n",
      "     |      \n",
      "     |          .. note::\n",
      "     |      \n",
      "     |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      "     |              like SHAP interaction values,\n",
      "     |              you can install the shap package (https://github.com/slundberg/shap).\n",
      "     |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      "     |              column, where the last column is the expected value.\n",
      "     |      \n",
      "     |      data_has_header : bool, optional (default=False)\n",
      "     |          Whether the data has header.\n",
      "     |          Used only if data is string.\n",
      "     |      is_reshape : bool, optional (default=True)\n",
      "     |          If True, result is reshaped to [nrow, ncol].\n",
      "     |      **kwargs\n",
      "     |          Other parameters for the prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : numpy array, scipy.sparse or list of scipy.sparse\n",
      "     |          Prediction result.\n",
      "     |          Can be sparse or a list of sparse objects (each element represents predictions for one class) for feature contributions (when ``pred_contrib=True``).\n",
      "     |  \n",
      "     |  refit(self, data, label, decay_rate=0.9, **kwargs)\n",
      "     |      Refit the existing Booster by new data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : string, numpy array, pandas DataFrame, H2O DataTable's Frame or scipy.sparse\n",
      "     |          Data source for refit.\n",
      "     |          If string, it represents the path to txt file.\n",
      "     |      label : list, numpy 1-D array or pandas Series / one-column DataFrame\n",
      "     |          Label for refit.\n",
      "     |      decay_rate : float, optional (default=0.9)\n",
      "     |          Decay rate of refit,\n",
      "     |          will use ``leaf_output = decay_rate * old_leaf_output + (1.0 - decay_rate) * new_leaf_output`` to refit trees.\n",
      "     |      **kwargs\n",
      "     |          Other parameters for refit.\n",
      "     |          These parameters will be passed to ``predict`` method.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : Booster\n",
      "     |          Refitted Booster.\n",
      "     |  \n",
      "     |  reset_parameter(self, params)\n",
      "     |      Reset parameters of Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params : dict\n",
      "     |          New parameters for Booster.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Booster with new parameters.\n",
      "     |  \n",
      "     |  rollback_one_iter(self)\n",
      "     |      Rollback one iteration.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Booster with rolled back one iteration.\n",
      "     |  \n",
      "     |  save_model(self, filename, num_iteration=None, start_iteration=0, importance_type='split')\n",
      "     |      Save Booster to file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      filename : string\n",
      "     |          Filename to save Booster.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Index of the iteration that should be saved.\n",
      "     |          If None, if the best iteration exists, it is saved; otherwise, all iterations are saved.\n",
      "     |          If <= 0, all iterations are saved.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration that should be saved.\n",
      "     |      importance_type : string, optional (default=\"split\")\n",
      "     |          What type of feature importance should be saved.\n",
      "     |          If \"split\", result contains numbers of times the feature is used in a model.\n",
      "     |          If \"gain\", result contains total gains of splits which use the feature.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  set_attr(self, **kwargs)\n",
      "     |      Set attributes to the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs\n",
      "     |          The attributes to set.\n",
      "     |          Setting a value to None deletes an attribute.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Booster with set attributes.\n",
      "     |  \n",
      "     |  set_network(self, machines: Union[List[str], Set[str], str], local_listen_port: int = 12400, listen_time_out: int = 120, num_machines: int = 1) -> 'Booster'\n",
      "     |      Set the network configuration.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      machines : list, set or string\n",
      "     |          Names of machines.\n",
      "     |      local_listen_port : int, optional (default=12400)\n",
      "     |          TCP listen port for local machines.\n",
      "     |      listen_time_out : int, optional (default=120)\n",
      "     |          Socket time-out in minutes.\n",
      "     |      num_machines : int, optional (default=1)\n",
      "     |          The number of machines for distributed learning application.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Booster with set network.\n",
      "     |  \n",
      "     |  set_train_data_name(self, name)\n",
      "     |      Set the name to the training Dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : string\n",
      "     |          Name for the training Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Booster with set training Dataset name.\n",
      "     |  \n",
      "     |  shuffle_models(self, start_iteration=0, end_iteration=-1)\n",
      "     |      Shuffle models.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          The first iteration that will be shuffled.\n",
      "     |      end_iteration : int, optional (default=-1)\n",
      "     |          The last iteration that will be shuffled.\n",
      "     |          If <= 0, means the last available iteration.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Booster\n",
      "     |          Booster with shuffled models.\n",
      "     |  \n",
      "     |  trees_to_dataframe(self)\n",
      "     |      Parse the fitted model and return in an easy-to-read pandas DataFrame.\n",
      "     |      \n",
      "     |      The returned DataFrame has the following columns.\n",
      "     |      \n",
      "     |          - ``tree_index`` : int64, which tree a node belongs to. 0-based, so a value of ``6``, for example, means \"this node is in the 7th tree\".\n",
      "     |          - ``node_depth`` : int64, how far a node is from the root of the tree. The root node has a value of ``1``, its direct children are ``2``, etc.\n",
      "     |          - ``node_index`` : string, unique identifier for a node.\n",
      "     |          - ``left_child`` : string, ``node_index`` of the child node to the left of a split. ``None`` for leaf nodes.\n",
      "     |          - ``right_child`` : string, ``node_index`` of the child node to the right of a split. ``None`` for leaf nodes.\n",
      "     |          - ``parent_index`` : string, ``node_index`` of this node's parent. ``None`` for the root node.\n",
      "     |          - ``split_feature`` : string, name of the feature used for splitting. ``None`` for leaf nodes.\n",
      "     |          - ``split_gain`` : float64, gain from adding this split to the tree. ``NaN`` for leaf nodes.\n",
      "     |          - ``threshold`` : float64, value of the feature used to decide which side of the split a record will go down. ``NaN`` for leaf nodes.\n",
      "     |          - ``decision_type`` : string, logical operator describing how to compare a value to ``threshold``.\n",
      "     |            For example, ``split_feature = \"Column_10\", threshold = 15, decision_type = \"<=\"`` means that\n",
      "     |            records where ``Column_10 <= 15`` follow the left side of the split, otherwise follows the right side of the split. ``None`` for leaf nodes.\n",
      "     |          - ``missing_direction`` : string, split direction that missing values should go to. ``None`` for leaf nodes.\n",
      "     |          - ``missing_type`` : string, describes what types of values are treated as missing.\n",
      "     |          - ``value`` : float64, predicted value for this leaf node, multiplied by the learning rate.\n",
      "     |          - ``weight`` : float64 or int64, sum of hessian (second-order derivative of objective), summed over observations that fall in this node.\n",
      "     |          - ``count`` : int64, number of records in the training data that fall into this node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : pandas DataFrame\n",
      "     |          Returns a pandas DataFrame of the parsed model.\n",
      "     |  \n",
      "     |  update(self, train_set=None, fobj=None)\n",
      "     |      Update Booster for one iteration.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      train_set : Dataset or None, optional (default=None)\n",
      "     |          Training data.\n",
      "     |          If None, last training data is used.\n",
      "     |      fobj : callable or None, optional (default=None)\n",
      "     |          Customized objective function.\n",
      "     |          Should accept two parameters: preds, train_data,\n",
      "     |          and return (grad, hess).\n",
      "     |      \n",
      "     |              preds : list or numpy 1-D array\n",
      "     |                  The predicted values.\n",
      "     |              train_data : Dataset\n",
      "     |                  The training dataset.\n",
      "     |              grad : list or numpy 1-D array\n",
      "     |                  The value of the first order derivative (gradient) for each sample point.\n",
      "     |              hess : list or numpy 1-D array\n",
      "     |                  The value of the second order derivative (Hessian) for each sample point.\n",
      "     |      \n",
      "     |          For binary task, the preds is probability of positive class (or margin in case of specified ``fobj``).\n",
      "     |          For multi-class task, the preds is group by class_id first, then group by row_id.\n",
      "     |          If you want to get i-th row preds in j-th class, the access way is score[j * num_data + i]\n",
      "     |          and you should group grad and hess in this way as well.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_finished : bool\n",
      "     |          Whether the update was successfully finished.\n",
      "     |  \n",
      "     |  upper_bound(self)\n",
      "     |      Get upper bound value of a model.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      upper_bound : double\n",
      "     |          Upper bound value of the model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CVBooster(builtins.object)\n",
      "     |  CVBooster in LightGBM.\n",
      "     |  \n",
      "     |  Auxiliary data structure to hold and redirect all boosters of ``cv`` function.\n",
      "     |  This class has the same methods as Booster class.\n",
      "     |  All method calls are actually performed for underlying Boosters and then all returned results are returned in a list.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  boosters : list of Booster\n",
      "     |      The list of underlying fitted models.\n",
      "     |  best_iteration : int\n",
      "     |      The best iteration of fitted model.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |      Redirect methods call of CVBooster.\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize the CVBooster.\n",
      "     |      \n",
      "     |      Generally, no need to instantiate manually.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DaskLGBMClassifier(lightgbm.sklearn.LGBMClassifier, _DaskLGBMModel)\n",
      "     |  DaskLGBMClassifier(boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[Callable, str, NoneType] = None, class_weight: Union[dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: int = -1, silent: bool = True, importance_type: str = 'split', client: Union[object, NoneType] = None, **kwargs: Any)\n",
      "     |  \n",
      "     |  Distributed version of lightgbm.LGBMClassifier.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DaskLGBMClassifier\n",
      "     |      lightgbm.sklearn.LGBMClassifier\n",
      "     |      lightgbm.sklearn.LGBMModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _DaskLGBMModel\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(self) -> Dict[Any, Any]\n",
      "     |  \n",
      "     |  __init__(self, boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[Callable, str, NoneType] = None, class_weight: Union[dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: int = -1, silent: bool = True, importance_type: str = 'split', client: Union[object, NoneType] = None, **kwargs: Any)\n",
      "     |      Construct a gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      boosting_type : string, optional (default='gbdt')\n",
      "     |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      "     |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      "     |          'goss', Gradient-based One-Side Sampling.\n",
      "     |          'rf', Random Forest.\n",
      "     |      num_leaves : int, optional (default=31)\n",
      "     |          Maximum tree leaves for base learners.\n",
      "     |      max_depth : int, optional (default=-1)\n",
      "     |          Maximum tree depth for base learners, <=0 means no limit.\n",
      "     |      learning_rate : float, optional (default=0.1)\n",
      "     |          Boosting learning rate.\n",
      "     |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      "     |          in training using ``reset_parameter`` callback.\n",
      "     |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      "     |      n_estimators : int, optional (default=100)\n",
      "     |          Number of boosted trees to fit.\n",
      "     |      subsample_for_bin : int, optional (default=200000)\n",
      "     |          Number of samples for constructing bins.\n",
      "     |      objective : string, callable or None, optional (default=None)\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      "     |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      "     |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |          Use this parameter only for multi-class classification task;\n",
      "     |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      "     |          Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      "     |          You may want to consider performing probability calibration\n",
      "     |          (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      "     |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      "     |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |          If None, all classes are supposed to have weight one.\n",
      "     |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      "     |          if ``sample_weight`` is specified.\n",
      "     |      min_split_gain : float, optional (default=0.)\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |      min_child_weight : float, optional (default=1e-3)\n",
      "     |          Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
      "     |      min_child_samples : int, optional (default=20)\n",
      "     |          Minimum number of data needed in a child (leaf).\n",
      "     |      subsample : float, optional (default=1.)\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      subsample_freq : int, optional (default=0)\n",
      "     |          Frequence of subsample, <=0 means no enable.\n",
      "     |      colsample_bytree : float, optional (default=1.)\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      reg_alpha : float, optional (default=0.)\n",
      "     |          L1 regularization term on weights.\n",
      "     |      reg_lambda : float, optional (default=0.)\n",
      "     |          L2 regularization term on weights.\n",
      "     |      random_state : int, RandomState object or None, optional (default=None)\n",
      "     |          Random number seed.\n",
      "     |          If int, this number is used to seed the C++ code.\n",
      "     |          If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      "     |          If None, default seeds in C++ code are used.\n",
      "     |      n_jobs : int, optional (default=-1)\n",
      "     |          Number of parallel threads.\n",
      "     |      silent : bool, optional (default=True)\n",
      "     |          Whether to print messages while running boosting.\n",
      "     |      importance_type : string, optional (default='split')\n",
      "     |          The type of feature importance to be filled into ``feature_importances_``.\n",
      "     |          If 'split', result contains numbers of times the feature is used in a model.\n",
      "     |          If 'gain', result contains total gains of splits which use the feature.\n",
      "     |      client : dask.distributed.Client or None, optional (default=None)\n",
      "     |          Dask client. If ``None``, ``distributed.default_client()`` will be used at runtime. The Dask client used by this class will not be saved if the model object is pickled.\n",
      "     |      **kwargs\n",
      "     |          Other parameters for the model.\n",
      "     |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      "     |      \n",
      "     |          .. warning::\n",
      "     |      \n",
      "     |              \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      "     |  \n",
      "     |  fit(self, X: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame], y: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame, lightgbm.compat.dask_Series], sample_weight: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame, lightgbm.compat.dask_Series, NoneType] = None, init_score: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame, lightgbm.compat.dask_Series, NoneType] = None, **kwargs: Any) -> 'DaskLGBMClassifier'\n",
      "     |      Build a gradient boosting model from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : Dask Array or Dask DataFrame of shape = [n_samples, n_features]\n",
      "     |          Input feature matrix.\n",
      "     |      y : Dask Array, Dask DataFrame or Dask Series of shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in regression).\n",
      "     |      sample_weight : Dask Array, Dask DataFrame, Dask Series of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Weights of training data.\n",
      "     |      init_score : Dask Array, Dask DataFrame, Dask Series of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Init score of training data.\n",
      "     |      verbose : bool or int, optional (default=True)\n",
      "     |          Requires at least one evaluation data.\n",
      "     |          If True, the eval metric on the eval set is printed at each boosting stage.\n",
      "     |          If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n",
      "     |          The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      "     |      \n",
      "     |          .. rubric:: Example\n",
      "     |      \n",
      "     |          With ``verbose`` = 4 and at least one item in ``eval_set``,\n",
      "     |          an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      "     |      \n",
      "     |      feature_name : list of strings or 'auto', optional (default='auto')\n",
      "     |          Feature names.\n",
      "     |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "     |      categorical_feature : list of strings or int, or 'auto', optional (default='auto')\n",
      "     |          Categorical features.\n",
      "     |          If list of int, interpreted as indices.\n",
      "     |          If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "     |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "     |          All values in categorical features should be less than int32 max value (2147483647).\n",
      "     |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "     |          All negative values in categorical features will be treated as missing values.\n",
      "     |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "     |      **kwargs\n",
      "     |              Other parameters passed through to ``LGBMClassifier.fit()``.\n",
      "     |  \n",
      "     |  predict(self, X: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame], **kwargs: Any) -> lightgbm.compat.dask_Array\n",
      "     |      Return the predicted value for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : Dask Array or Dask DataFrame of shape = [n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      raw_score : bool, optional (default=False)\n",
      "     |          Whether to predict raw scores.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration to predict.\n",
      "     |          If <= 0, starts from the first iteration.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Total number of iterations used in the prediction.\n",
      "     |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      "     |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |      pred_leaf : bool, optional (default=False)\n",
      "     |          Whether to predict leaf index.\n",
      "     |      pred_contrib : bool, optional (default=False)\n",
      "     |          Whether to predict feature contributions.\n",
      "     |      \n",
      "     |          .. note::\n",
      "     |      \n",
      "     |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      "     |              like SHAP interaction values,\n",
      "     |              you can install the shap package (https://github.com/slundberg/shap).\n",
      "     |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      "     |              column, where the last column is the expected value.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Other parameters for the prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predicted_result : Dask Array of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      "     |          The predicted values.\n",
      "     |      X_leaves : Dask Array of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      "     |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      "     |      X_SHAP_values : Dask Array of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes]\n",
      "     |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      "     |  \n",
      "     |  predict_proba(self, X: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame], **kwargs: Any) -> lightgbm.compat.dask_Array\n",
      "     |      Return the predicted probability for each class for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : Dask Array or Dask DataFrame of shape = [n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      raw_score : bool, optional (default=False)\n",
      "     |          Whether to predict raw scores.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration to predict.\n",
      "     |          If <= 0, starts from the first iteration.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Total number of iterations used in the prediction.\n",
      "     |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      "     |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |      pred_leaf : bool, optional (default=False)\n",
      "     |          Whether to predict leaf index.\n",
      "     |      pred_contrib : bool, optional (default=False)\n",
      "     |          Whether to predict feature contributions.\n",
      "     |      \n",
      "     |          .. note::\n",
      "     |      \n",
      "     |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      "     |              like SHAP interaction values,\n",
      "     |              you can install the shap package (https://github.com/slundberg/shap).\n",
      "     |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      "     |              column, where the last column is the expected value.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Other parameters for the prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predicted_probability : Dask Array of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      "     |          The predicted values.\n",
      "     |      X_leaves : Dask Array of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      "     |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      "     |      X_SHAP_values : Dask Array of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes]\n",
      "     |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      "     |  \n",
      "     |  to_local(self) -> lightgbm.sklearn.LGBMClassifier\n",
      "     |      Create regular version of lightgbm.LGBMClassifier from the distributed version.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      model : lightgbm.LGBMClassifier\n",
      "     |          Local underlying model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from lightgbm.sklearn.LGBMClassifier:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      :obj:`array` of shape = [n_classes]: The class label array.\n",
      "     |  \n",
      "     |  n_classes_\n",
      "     |      :obj:`int`: The number of classes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from lightgbm.sklearn.LGBMModel:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, optional (default=True)\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params\n",
      "     |          Parameter names with their new values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from lightgbm.sklearn.LGBMModel:\n",
      "     |  \n",
      "     |  best_iteration_\n",
      "     |      :obj:`int` or :obj:`None`: The best iteration of fitted model if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  best_score_\n",
      "     |      :obj:`dict` or :obj:`None`: The best score of fitted model.\n",
      "     |  \n",
      "     |  booster_\n",
      "     |      Booster: The underlying Booster of this model.\n",
      "     |  \n",
      "     |  evals_result_\n",
      "     |      :obj:`dict` or :obj:`None`: The evaluation results if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      :obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |      \n",
      "     |          ``importance_type`` attribute is passed to the function\n",
      "     |          to configure the type of importance values to be extracted.\n",
      "     |  \n",
      "     |  feature_name_\n",
      "     |      :obj:`array` of shape = [n_features]: The names of features.\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  objective_\n",
      "     |      :obj:`string` or :obj:`callable`: The concrete objective used while fitting this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _DaskLGBMModel:\n",
      "     |  \n",
      "     |  client_\n",
      "     |      :obj:`dask.distributed.Client`: Dask client.\n",
      "     |      \n",
      "     |      This property can be passed in the constructor or updated\n",
      "     |      with ``model.set_params(client=client)``.\n",
      "    \n",
      "    class DaskLGBMRanker(lightgbm.sklearn.LGBMRanker, _DaskLGBMModel)\n",
      "     |  DaskLGBMRanker(boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[Callable, str, NoneType] = None, class_weight: Union[dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: int = -1, silent: bool = True, importance_type: str = 'split', client: Union[object, NoneType] = None, **kwargs: Any)\n",
      "     |  \n",
      "     |  Distributed version of lightgbm.LGBMRanker.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DaskLGBMRanker\n",
      "     |      lightgbm.sklearn.LGBMRanker\n",
      "     |      lightgbm.sklearn.LGBMModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      _DaskLGBMModel\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(self) -> Dict[Any, Any]\n",
      "     |  \n",
      "     |  __init__(self, boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[Callable, str, NoneType] = None, class_weight: Union[dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: int = -1, silent: bool = True, importance_type: str = 'split', client: Union[object, NoneType] = None, **kwargs: Any)\n",
      "     |      Construct a gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      boosting_type : string, optional (default='gbdt')\n",
      "     |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      "     |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      "     |          'goss', Gradient-based One-Side Sampling.\n",
      "     |          'rf', Random Forest.\n",
      "     |      num_leaves : int, optional (default=31)\n",
      "     |          Maximum tree leaves for base learners.\n",
      "     |      max_depth : int, optional (default=-1)\n",
      "     |          Maximum tree depth for base learners, <=0 means no limit.\n",
      "     |      learning_rate : float, optional (default=0.1)\n",
      "     |          Boosting learning rate.\n",
      "     |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      "     |          in training using ``reset_parameter`` callback.\n",
      "     |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      "     |      n_estimators : int, optional (default=100)\n",
      "     |          Number of boosted trees to fit.\n",
      "     |      subsample_for_bin : int, optional (default=200000)\n",
      "     |          Number of samples for constructing bins.\n",
      "     |      objective : string, callable or None, optional (default=None)\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      "     |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      "     |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |          Use this parameter only for multi-class classification task;\n",
      "     |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      "     |          Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      "     |          You may want to consider performing probability calibration\n",
      "     |          (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      "     |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      "     |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |          If None, all classes are supposed to have weight one.\n",
      "     |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      "     |          if ``sample_weight`` is specified.\n",
      "     |      min_split_gain : float, optional (default=0.)\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |      min_child_weight : float, optional (default=1e-3)\n",
      "     |          Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
      "     |      min_child_samples : int, optional (default=20)\n",
      "     |          Minimum number of data needed in a child (leaf).\n",
      "     |      subsample : float, optional (default=1.)\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      subsample_freq : int, optional (default=0)\n",
      "     |          Frequence of subsample, <=0 means no enable.\n",
      "     |      colsample_bytree : float, optional (default=1.)\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      reg_alpha : float, optional (default=0.)\n",
      "     |          L1 regularization term on weights.\n",
      "     |      reg_lambda : float, optional (default=0.)\n",
      "     |          L2 regularization term on weights.\n",
      "     |      random_state : int, RandomState object or None, optional (default=None)\n",
      "     |          Random number seed.\n",
      "     |          If int, this number is used to seed the C++ code.\n",
      "     |          If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      "     |          If None, default seeds in C++ code are used.\n",
      "     |      n_jobs : int, optional (default=-1)\n",
      "     |          Number of parallel threads.\n",
      "     |      silent : bool, optional (default=True)\n",
      "     |          Whether to print messages while running boosting.\n",
      "     |      importance_type : string, optional (default='split')\n",
      "     |          The type of feature importance to be filled into ``feature_importances_``.\n",
      "     |          If 'split', result contains numbers of times the feature is used in a model.\n",
      "     |          If 'gain', result contains total gains of splits which use the feature.\n",
      "     |      client : dask.distributed.Client or None, optional (default=None)\n",
      "     |          Dask client. If ``None``, ``distributed.default_client()`` will be used at runtime. The Dask client used by this class will not be saved if the model object is pickled.\n",
      "     |      **kwargs\n",
      "     |          Other parameters for the model.\n",
      "     |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      "     |      \n",
      "     |          .. warning::\n",
      "     |      \n",
      "     |              \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      "     |  \n",
      "     |  fit(self, X: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame], y: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame, lightgbm.compat.dask_Series], sample_weight: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame, lightgbm.compat.dask_Series, NoneType] = None, init_score: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame, lightgbm.compat.dask_Series, NoneType] = None, group: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame, lightgbm.compat.dask_Series, NoneType] = None, **kwargs: Any) -> 'DaskLGBMRanker'\n",
      "     |      Build a gradient boosting model from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : Dask Array or Dask DataFrame of shape = [n_samples, n_features]\n",
      "     |          Input feature matrix.\n",
      "     |      y : Dask Array, Dask DataFrame or Dask Series of shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in regression).\n",
      "     |      sample_weight : Dask Array, Dask DataFrame, Dask Series of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Weights of training data.\n",
      "     |      init_score : Dask Array, Dask DataFrame, Dask Series of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Init score of training data.\n",
      "     |      group : Dask Array, Dask DataFrame, Dask Series of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Group/query data.\n",
      "     |          Only used in the learning-to-rank task.\n",
      "     |          sum(group) = n_samples.\n",
      "     |          For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |          where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |      verbose : bool or int, optional (default=True)\n",
      "     |          Requires at least one evaluation data.\n",
      "     |          If True, the eval metric on the eval set is printed at each boosting stage.\n",
      "     |          If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n",
      "     |          The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      "     |      \n",
      "     |          .. rubric:: Example\n",
      "     |      \n",
      "     |          With ``verbose`` = 4 and at least one item in ``eval_set``,\n",
      "     |          an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      "     |      \n",
      "     |      feature_name : list of strings or 'auto', optional (default='auto')\n",
      "     |          Feature names.\n",
      "     |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "     |      categorical_feature : list of strings or int, or 'auto', optional (default='auto')\n",
      "     |          Categorical features.\n",
      "     |          If list of int, interpreted as indices.\n",
      "     |          If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "     |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "     |          All values in categorical features should be less than int32 max value (2147483647).\n",
      "     |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "     |          All negative values in categorical features will be treated as missing values.\n",
      "     |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "     |      **kwargs\n",
      "     |              Other parameters passed through to ``LGBMRanker.fit()``.\n",
      "     |  \n",
      "     |  predict(self, X: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame], **kwargs: Any) -> lightgbm.compat.dask_Array\n",
      "     |      Return the predicted value for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : Dask Array or Dask DataFrame of shape = [n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      raw_score : bool, optional (default=False)\n",
      "     |          Whether to predict raw scores.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration to predict.\n",
      "     |          If <= 0, starts from the first iteration.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Total number of iterations used in the prediction.\n",
      "     |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      "     |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |      pred_leaf : bool, optional (default=False)\n",
      "     |          Whether to predict leaf index.\n",
      "     |      pred_contrib : bool, optional (default=False)\n",
      "     |          Whether to predict feature contributions.\n",
      "     |      \n",
      "     |          .. note::\n",
      "     |      \n",
      "     |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      "     |              like SHAP interaction values,\n",
      "     |              you can install the shap package (https://github.com/slundberg/shap).\n",
      "     |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      "     |              column, where the last column is the expected value.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Other parameters for the prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predicted_result : Dask Array of shape = [n_samples]\n",
      "     |          The predicted values.\n",
      "     |      X_leaves : Dask Array of shape = [n_samples, n_trees]\n",
      "     |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      "     |      X_SHAP_values : Dask Array of shape = [n_samples, n_features + 1]\n",
      "     |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      "     |  \n",
      "     |  to_local(self) -> lightgbm.sklearn.LGBMRanker\n",
      "     |      Create regular version of lightgbm.LGBMRanker from the distributed version.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      model : lightgbm.LGBMRanker\n",
      "     |          Local underlying model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from lightgbm.sklearn.LGBMModel:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, optional (default=True)\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params\n",
      "     |          Parameter names with their new values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from lightgbm.sklearn.LGBMModel:\n",
      "     |  \n",
      "     |  best_iteration_\n",
      "     |      :obj:`int` or :obj:`None`: The best iteration of fitted model if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  best_score_\n",
      "     |      :obj:`dict` or :obj:`None`: The best score of fitted model.\n",
      "     |  \n",
      "     |  booster_\n",
      "     |      Booster: The underlying Booster of this model.\n",
      "     |  \n",
      "     |  evals_result_\n",
      "     |      :obj:`dict` or :obj:`None`: The evaluation results if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      :obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |      \n",
      "     |          ``importance_type`` attribute is passed to the function\n",
      "     |          to configure the type of importance values to be extracted.\n",
      "     |  \n",
      "     |  feature_name_\n",
      "     |      :obj:`array` of shape = [n_features]: The names of features.\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  objective_\n",
      "     |      :obj:`string` or :obj:`callable`: The concrete objective used while fitting this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _DaskLGBMModel:\n",
      "     |  \n",
      "     |  client_\n",
      "     |      :obj:`dask.distributed.Client`: Dask client.\n",
      "     |      \n",
      "     |      This property can be passed in the constructor or updated\n",
      "     |      with ``model.set_params(client=client)``.\n",
      "    \n",
      "    class DaskLGBMRegressor(lightgbm.sklearn.LGBMRegressor, _DaskLGBMModel)\n",
      "     |  DaskLGBMRegressor(boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[Callable, str, NoneType] = None, class_weight: Union[dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: int = -1, silent: bool = True, importance_type: str = 'split', client: Union[object, NoneType] = None, **kwargs: Any)\n",
      "     |  \n",
      "     |  Distributed version of lightgbm.LGBMRegressor.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DaskLGBMRegressor\n",
      "     |      lightgbm.sklearn.LGBMRegressor\n",
      "     |      lightgbm.sklearn.LGBMModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _DaskLGBMModel\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(self) -> Dict[Any, Any]\n",
      "     |  \n",
      "     |  __init__(self, boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[Callable, str, NoneType] = None, class_weight: Union[dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: int = -1, silent: bool = True, importance_type: str = 'split', client: Union[object, NoneType] = None, **kwargs: Any)\n",
      "     |      Construct a gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      boosting_type : string, optional (default='gbdt')\n",
      "     |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      "     |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      "     |          'goss', Gradient-based One-Side Sampling.\n",
      "     |          'rf', Random Forest.\n",
      "     |      num_leaves : int, optional (default=31)\n",
      "     |          Maximum tree leaves for base learners.\n",
      "     |      max_depth : int, optional (default=-1)\n",
      "     |          Maximum tree depth for base learners, <=0 means no limit.\n",
      "     |      learning_rate : float, optional (default=0.1)\n",
      "     |          Boosting learning rate.\n",
      "     |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      "     |          in training using ``reset_parameter`` callback.\n",
      "     |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      "     |      n_estimators : int, optional (default=100)\n",
      "     |          Number of boosted trees to fit.\n",
      "     |      subsample_for_bin : int, optional (default=200000)\n",
      "     |          Number of samples for constructing bins.\n",
      "     |      objective : string, callable or None, optional (default=None)\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      "     |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      "     |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |          Use this parameter only for multi-class classification task;\n",
      "     |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      "     |          Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      "     |          You may want to consider performing probability calibration\n",
      "     |          (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      "     |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      "     |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |          If None, all classes are supposed to have weight one.\n",
      "     |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      "     |          if ``sample_weight`` is specified.\n",
      "     |      min_split_gain : float, optional (default=0.)\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |      min_child_weight : float, optional (default=1e-3)\n",
      "     |          Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
      "     |      min_child_samples : int, optional (default=20)\n",
      "     |          Minimum number of data needed in a child (leaf).\n",
      "     |      subsample : float, optional (default=1.)\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      subsample_freq : int, optional (default=0)\n",
      "     |          Frequence of subsample, <=0 means no enable.\n",
      "     |      colsample_bytree : float, optional (default=1.)\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      reg_alpha : float, optional (default=0.)\n",
      "     |          L1 regularization term on weights.\n",
      "     |      reg_lambda : float, optional (default=0.)\n",
      "     |          L2 regularization term on weights.\n",
      "     |      random_state : int, RandomState object or None, optional (default=None)\n",
      "     |          Random number seed.\n",
      "     |          If int, this number is used to seed the C++ code.\n",
      "     |          If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      "     |          If None, default seeds in C++ code are used.\n",
      "     |      n_jobs : int, optional (default=-1)\n",
      "     |          Number of parallel threads.\n",
      "     |      silent : bool, optional (default=True)\n",
      "     |          Whether to print messages while running boosting.\n",
      "     |      importance_type : string, optional (default='split')\n",
      "     |          The type of feature importance to be filled into ``feature_importances_``.\n",
      "     |          If 'split', result contains numbers of times the feature is used in a model.\n",
      "     |          If 'gain', result contains total gains of splits which use the feature.\n",
      "     |      client : dask.distributed.Client or None, optional (default=None)\n",
      "     |          Dask client. If ``None``, ``distributed.default_client()`` will be used at runtime. The Dask client used by this class will not be saved if the model object is pickled.\n",
      "     |      **kwargs\n",
      "     |          Other parameters for the model.\n",
      "     |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      "     |      \n",
      "     |          .. warning::\n",
      "     |      \n",
      "     |              \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      "     |  \n",
      "     |  fit(self, X: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame], y: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame, lightgbm.compat.dask_Series], sample_weight: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame, lightgbm.compat.dask_Series, NoneType] = None, init_score: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame, lightgbm.compat.dask_Series, NoneType] = None, **kwargs: Any) -> 'DaskLGBMRegressor'\n",
      "     |      Build a gradient boosting model from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : Dask Array or Dask DataFrame of shape = [n_samples, n_features]\n",
      "     |          Input feature matrix.\n",
      "     |      y : Dask Array, Dask DataFrame or Dask Series of shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in regression).\n",
      "     |      sample_weight : Dask Array, Dask DataFrame, Dask Series of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Weights of training data.\n",
      "     |      init_score : Dask Array, Dask DataFrame, Dask Series of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Init score of training data.\n",
      "     |      verbose : bool or int, optional (default=True)\n",
      "     |          Requires at least one evaluation data.\n",
      "     |          If True, the eval metric on the eval set is printed at each boosting stage.\n",
      "     |          If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n",
      "     |          The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      "     |      \n",
      "     |          .. rubric:: Example\n",
      "     |      \n",
      "     |          With ``verbose`` = 4 and at least one item in ``eval_set``,\n",
      "     |          an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      "     |      \n",
      "     |      feature_name : list of strings or 'auto', optional (default='auto')\n",
      "     |          Feature names.\n",
      "     |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "     |      categorical_feature : list of strings or int, or 'auto', optional (default='auto')\n",
      "     |          Categorical features.\n",
      "     |          If list of int, interpreted as indices.\n",
      "     |          If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "     |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "     |          All values in categorical features should be less than int32 max value (2147483647).\n",
      "     |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "     |          All negative values in categorical features will be treated as missing values.\n",
      "     |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "     |      **kwargs\n",
      "     |              Other parameters passed through to ``LGBMRegressor.fit()``.\n",
      "     |  \n",
      "     |  predict(self, X: Union[lightgbm.compat.dask_Array, lightgbm.compat.dask_DataFrame], **kwargs) -> lightgbm.compat.dask_Array\n",
      "     |      Return the predicted value for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : Dask Array or Dask DataFrame of shape = [n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      raw_score : bool, optional (default=False)\n",
      "     |          Whether to predict raw scores.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration to predict.\n",
      "     |          If <= 0, starts from the first iteration.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Total number of iterations used in the prediction.\n",
      "     |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      "     |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |      pred_leaf : bool, optional (default=False)\n",
      "     |          Whether to predict leaf index.\n",
      "     |      pred_contrib : bool, optional (default=False)\n",
      "     |          Whether to predict feature contributions.\n",
      "     |      \n",
      "     |          .. note::\n",
      "     |      \n",
      "     |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      "     |              like SHAP interaction values,\n",
      "     |              you can install the shap package (https://github.com/slundberg/shap).\n",
      "     |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      "     |              column, where the last column is the expected value.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Other parameters for the prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predicted_result : Dask Array of shape = [n_samples]\n",
      "     |          The predicted values.\n",
      "     |      X_leaves : Dask Array of shape = [n_samples, n_trees]\n",
      "     |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      "     |      X_SHAP_values : Dask Array of shape = [n_samples, n_features + 1]\n",
      "     |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      "     |  \n",
      "     |  to_local(self) -> lightgbm.sklearn.LGBMRegressor\n",
      "     |      Create regular version of lightgbm.LGBMRegressor from the distributed version.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      model : lightgbm.LGBMRegressor\n",
      "     |          Local underlying model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from lightgbm.sklearn.LGBMModel:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, optional (default=True)\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params\n",
      "     |          Parameter names with their new values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from lightgbm.sklearn.LGBMModel:\n",
      "     |  \n",
      "     |  best_iteration_\n",
      "     |      :obj:`int` or :obj:`None`: The best iteration of fitted model if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  best_score_\n",
      "     |      :obj:`dict` or :obj:`None`: The best score of fitted model.\n",
      "     |  \n",
      "     |  booster_\n",
      "     |      Booster: The underlying Booster of this model.\n",
      "     |  \n",
      "     |  evals_result_\n",
      "     |      :obj:`dict` or :obj:`None`: The evaluation results if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      :obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |      \n",
      "     |          ``importance_type`` attribute is passed to the function\n",
      "     |          to configure the type of importance values to be extracted.\n",
      "     |  \n",
      "     |  feature_name_\n",
      "     |      :obj:`array` of shape = [n_features]: The names of features.\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  objective_\n",
      "     |      :obj:`string` or :obj:`callable`: The concrete objective used while fitting this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _DaskLGBMModel:\n",
      "     |  \n",
      "     |  client_\n",
      "     |      :obj:`dask.distributed.Client`: Dask client.\n",
      "     |      \n",
      "     |      This property can be passed in the constructor or updated\n",
      "     |      with ``model.set_params(client=client)``.\n",
      "    \n",
      "    class Dataset(builtins.object)\n",
      "     |  Dataset(data, label=None, reference=None, weight=None, group=None, init_score=None, silent=False, feature_name='auto', categorical_feature='auto', params=None, free_raw_data=True)\n",
      "     |  \n",
      "     |  Dataset in LightGBM.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __init__(self, data, label=None, reference=None, weight=None, group=None, init_score=None, silent=False, feature_name='auto', categorical_feature='auto', params=None, free_raw_data=True)\n",
      "     |      Initialize Dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : string, numpy array, pandas DataFrame, H2O DataTable's Frame, scipy.sparse or list of numpy arrays\n",
      "     |          Data source of Dataset.\n",
      "     |          If string, it represents the path to txt file.\n",
      "     |      label : list, numpy 1-D array, pandas Series / one-column DataFrame or None, optional (default=None)\n",
      "     |          Label of the data.\n",
      "     |      reference : Dataset or None, optional (default=None)\n",
      "     |          If this is Dataset for validation, training data should be used as reference.\n",
      "     |      weight : list, numpy 1-D array, pandas Series or None, optional (default=None)\n",
      "     |          Weight for each instance.\n",
      "     |      group : list, numpy 1-D array, pandas Series or None, optional (default=None)\n",
      "     |          Group/query data.\n",
      "     |          Only used in the learning-to-rank task.\n",
      "     |          sum(group) = n_samples.\n",
      "     |          For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |          where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |      init_score : list, numpy 1-D array, pandas Series or None, optional (default=None)\n",
      "     |          Init score for Dataset.\n",
      "     |      silent : bool, optional (default=False)\n",
      "     |          Whether to print messages during construction.\n",
      "     |      feature_name : list of strings or 'auto', optional (default=\"auto\")\n",
      "     |          Feature names.\n",
      "     |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "     |      categorical_feature : list of strings or int, or 'auto', optional (default=\"auto\")\n",
      "     |          Categorical features.\n",
      "     |          If list of int, interpreted as indices.\n",
      "     |          If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "     |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "     |          All values in categorical features should be less than int32 max value (2147483647).\n",
      "     |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "     |          All negative values in categorical features will be treated as missing values.\n",
      "     |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "     |      params : dict or None, optional (default=None)\n",
      "     |          Other parameters for Dataset.\n",
      "     |      free_raw_data : bool, optional (default=True)\n",
      "     |          If True, raw data is freed after constructing inner Dataset.\n",
      "     |  \n",
      "     |  add_features_from(self, other)\n",
      "     |      Add features from other Dataset to the current Dataset.\n",
      "     |      \n",
      "     |      Both Datasets must be constructed before calling this method.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : Dataset\n",
      "     |          The Dataset to take features from.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Dataset with the new features added.\n",
      "     |  \n",
      "     |  construct(self)\n",
      "     |      Lazy init.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Constructed Dataset object.\n",
      "     |  \n",
      "     |  create_valid(self, data, label=None, weight=None, group=None, init_score=None, silent=False, params=None)\n",
      "     |      Create validation data align with current Dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : string, numpy array, pandas DataFrame, H2O DataTable's Frame, scipy.sparse or list of numpy arrays\n",
      "     |          Data source of Dataset.\n",
      "     |          If string, it represents the path to txt file.\n",
      "     |      label : list, numpy 1-D array, pandas Series / one-column DataFrame or None, optional (default=None)\n",
      "     |          Label of the data.\n",
      "     |      weight : list, numpy 1-D array, pandas Series or None, optional (default=None)\n",
      "     |          Weight for each instance.\n",
      "     |      group : list, numpy 1-D array, pandas Series or None, optional (default=None)\n",
      "     |          Group/query data.\n",
      "     |          Only used in the learning-to-rank task.\n",
      "     |          sum(group) = n_samples.\n",
      "     |          For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |          where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |      init_score : list, numpy 1-D array, pandas Series or None, optional (default=None)\n",
      "     |          Init score for Dataset.\n",
      "     |      silent : bool, optional (default=False)\n",
      "     |          Whether to print messages during construction.\n",
      "     |      params : dict or None, optional (default=None)\n",
      "     |          Other parameters for validation Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      valid : Dataset\n",
      "     |          Validation Dataset with reference to self.\n",
      "     |  \n",
      "     |  get_data(self)\n",
      "     |      Get the raw data of the Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      data : string, numpy array, pandas DataFrame, H2O DataTable's Frame, scipy.sparse, list of numpy arrays or None\n",
      "     |          Raw data used in the Dataset construction.\n",
      "     |  \n",
      "     |  get_feature_name(self)\n",
      "     |      Get the names of columns (features) in the Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names : list\n",
      "     |          The names of columns (features) in the Dataset.\n",
      "     |  \n",
      "     |  get_field(self, field_name)\n",
      "     |      Get property from the Dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field_name : string\n",
      "     |          The field name of the information.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : numpy array\n",
      "     |          A numpy array with information from the Dataset.\n",
      "     |  \n",
      "     |  get_group(self)\n",
      "     |      Get the group of the Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      group : numpy array or None\n",
      "     |          Group/query data.\n",
      "     |          Only used in the learning-to-rank task.\n",
      "     |          sum(group) = n_samples.\n",
      "     |          For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |          where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |  \n",
      "     |  get_init_score(self)\n",
      "     |      Get the initial score of the Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      init_score : numpy array or None\n",
      "     |          Init score of Booster.\n",
      "     |  \n",
      "     |  get_label(self)\n",
      "     |      Get the label of the Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      label : numpy array or None\n",
      "     |          The label information from the Dataset.\n",
      "     |  \n",
      "     |  get_params(self)\n",
      "     |      Get the used parameters in the Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict or None\n",
      "     |          The used parameters in this Dataset object.\n",
      "     |  \n",
      "     |  get_ref_chain(self, ref_limit=100)\n",
      "     |      Get a chain of Dataset objects.\n",
      "     |      \n",
      "     |      Starts with r, then goes to r.reference (if exists),\n",
      "     |      then to r.reference.reference, etc.\n",
      "     |      until we hit ``ref_limit`` or a reference loop.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ref_limit : int, optional (default=100)\n",
      "     |          The limit number of references.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ref_chain : set of Dataset\n",
      "     |          Chain of references of the Datasets.\n",
      "     |  \n",
      "     |  get_weight(self)\n",
      "     |      Get the weight of the Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      weight : numpy array or None\n",
      "     |          Weight for each data point from the Dataset.\n",
      "     |  \n",
      "     |  num_data(self)\n",
      "     |      Get the number of rows in the Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number_of_rows : int\n",
      "     |          The number of rows in the Dataset.\n",
      "     |  \n",
      "     |  num_feature(self)\n",
      "     |      Get the number of columns (features) in the Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number_of_columns : int\n",
      "     |          The number of columns (features) in the Dataset.\n",
      "     |  \n",
      "     |  save_binary(self, filename)\n",
      "     |      Save Dataset to a binary file.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |      \n",
      "     |          Please note that `init_score` is not saved in binary file.\n",
      "     |          If you need it, please set it again after loading Dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      filename : string\n",
      "     |          Name of the output file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  set_categorical_feature(self, categorical_feature)\n",
      "     |      Set categorical features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      categorical_feature : list of int or strings\n",
      "     |          Names or indices of categorical features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Dataset with set categorical features.\n",
      "     |  \n",
      "     |  set_feature_name(self, feature_name)\n",
      "     |      Set feature name.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      feature_name : list of strings\n",
      "     |          Feature names.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Dataset with set feature name.\n",
      "     |  \n",
      "     |  set_field(self, field_name, data)\n",
      "     |      Set property into the Dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field_name : string\n",
      "     |          The field name of the information.\n",
      "     |      data : list, numpy 1-D array, pandas Series or None\n",
      "     |          The array of data to be set.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Dataset with set property.\n",
      "     |  \n",
      "     |  set_group(self, group)\n",
      "     |      Set group size of Dataset (used for ranking).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      group : list, numpy 1-D array, pandas Series or None\n",
      "     |          Group/query data.\n",
      "     |          Only used in the learning-to-rank task.\n",
      "     |          sum(group) = n_samples.\n",
      "     |          For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |          where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Dataset with set group.\n",
      "     |  \n",
      "     |  set_init_score(self, init_score)\n",
      "     |      Set init score of Booster to start from.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      init_score : list, numpy 1-D array, pandas Series or None\n",
      "     |          Init score for Booster.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Dataset with set init score.\n",
      "     |  \n",
      "     |  set_label(self, label)\n",
      "     |      Set label of Dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label : list, numpy 1-D array, pandas Series / one-column DataFrame or None\n",
      "     |          The label information to be set into Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Dataset with set label.\n",
      "     |  \n",
      "     |  set_reference(self, reference)\n",
      "     |      Set reference Dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      reference : Dataset\n",
      "     |          Reference that is used as a template to construct the current Dataset.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Dataset with set reference.\n",
      "     |  \n",
      "     |  set_weight(self, weight)\n",
      "     |      Set weight of each instance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : list, numpy 1-D array, pandas Series or None\n",
      "     |          Weight to be set for each data point.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Dataset\n",
      "     |          Dataset with set weight.\n",
      "     |  \n",
      "     |  subset(self, used_indices, params=None)\n",
      "     |      Get subset of current Dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      used_indices : list of int\n",
      "     |          Indices used to create the subset.\n",
      "     |      params : dict or None, optional (default=None)\n",
      "     |          These parameters will be passed to Dataset constructor.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      subset : Dataset\n",
      "     |          Subset of the current Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LGBMClassifier(LGBMModel, sklearn.base.ClassifierMixin)\n",
      "     |  LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs)\n",
      "     |  \n",
      "     |  LightGBM classifier.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LGBMClassifier\n",
      "     |      LGBMModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, init_score=None, eval_set=None, eval_names=None, eval_sample_weight=None, eval_class_weight=None, eval_init_score=None, eval_metric=None, early_stopping_rounds=None, verbose=True, feature_name='auto', categorical_feature='auto', callbacks=None, init_model=None)\n",
      "     |      Build a gradient boosting model from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          Input feature matrix.\n",
      "     |      y : array-like of shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in regression).\n",
      "     |      sample_weight : array-like of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Weights of training data.\n",
      "     |      init_score : array-like of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Init score of training data.\n",
      "     |      eval_set : list or None, optional (default=None)\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets.\n",
      "     |      eval_names : list of strings or None, optional (default=None)\n",
      "     |          Names of eval_set.\n",
      "     |      eval_sample_weight : list of arrays or None, optional (default=None)\n",
      "     |          Weights of eval data.\n",
      "     |      eval_class_weight : list or None, optional (default=None)\n",
      "     |          Class weights of eval data.\n",
      "     |      eval_init_score : list of arrays or None, optional (default=None)\n",
      "     |          Init score of eval data.\n",
      "     |      eval_metric : string, callable, list or None, optional (default=None)\n",
      "     |          If string, it should be a built-in evaluation metric to use.\n",
      "     |          If callable, it should be a custom evaluation metric, see note below for more details.\n",
      "     |          If list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both.\n",
      "     |          In either case, the ``metric`` from the model parameters will be evaluated and used as well.\n",
      "     |          Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker.\n",
      "     |      early_stopping_rounds : int or None, optional (default=None)\n",
      "     |          Activates early stopping. The model will train until the validation score stops improving.\n",
      "     |          Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
      "     |          to continue training.\n",
      "     |          Requires at least one validation data and one metric.\n",
      "     |          If there's more than one, will check all of them. But the training data is ignored anyway.\n",
      "     |          To check only the first metric, set the ``first_metric_only`` parameter to ``True``\n",
      "     |          in additional parameters ``**kwargs`` of the model constructor.\n",
      "     |      verbose : bool or int, optional (default=True)\n",
      "     |          Requires at least one evaluation data.\n",
      "     |          If True, the eval metric on the eval set is printed at each boosting stage.\n",
      "     |          If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n",
      "     |          The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      "     |      \n",
      "     |          .. rubric:: Example\n",
      "     |      \n",
      "     |          With ``verbose`` = 4 and at least one item in ``eval_set``,\n",
      "     |          an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      "     |      \n",
      "     |      feature_name : list of strings or 'auto', optional (default='auto')\n",
      "     |          Feature names.\n",
      "     |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "     |      categorical_feature : list of strings or int, or 'auto', optional (default='auto')\n",
      "     |          Categorical features.\n",
      "     |          If list of int, interpreted as indices.\n",
      "     |          If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "     |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "     |          All values in categorical features should be less than int32 max value (2147483647).\n",
      "     |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "     |          All negative values in categorical features will be treated as missing values.\n",
      "     |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "     |      callbacks : list of callback functions or None, optional (default=None)\n",
      "     |          List of callback functions that are applied at each iteration.\n",
      "     |          See Callbacks in Python API for more information.\n",
      "     |      init_model : string, Booster, LGBMModel or None, optional (default=None)\n",
      "     |          Filename of LightGBM model, Booster instance or LGBMModel instance used for continue training.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      Note\n",
      "     |      ----\n",
      "     |      Custom eval function expects a callable with following signatures:\n",
      "     |      ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n",
      "     |      ``func(y_true, y_pred, weight, group)``\n",
      "     |      and returns (eval_name, eval_result, is_higher_better) or\n",
      "     |      list of (eval_name, eval_result, is_higher_better):\n",
      "     |      \n",
      "     |          y_true : array-like of shape = [n_samples]\n",
      "     |              The target values.\n",
      "     |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The predicted values.\n",
      "     |          weight : array-like of shape = [n_samples]\n",
      "     |              The weight of samples.\n",
      "     |          group : array-like\n",
      "     |              Group/query data.\n",
      "     |              Only used in the learning-to-rank task.\n",
      "     |              sum(group) = n_samples.\n",
      "     |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |          eval_name : string\n",
      "     |              The name of evaluation function (without whitespaces).\n",
      "     |          eval_result : float\n",
      "     |              The eval result.\n",
      "     |          is_higher_better : bool\n",
      "     |              Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      "     |      \n",
      "     |      For binary task, the y_pred is probability of positive class (or margin in case of custom ``objective``).\n",
      "     |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      "     |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i].\n",
      "     |  \n",
      "     |  predict(self, X, raw_score=False, start_iteration=0, num_iteration=None, pred_leaf=False, pred_contrib=False, **kwargs)\n",
      "     |      Return the predicted value for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      raw_score : bool, optional (default=False)\n",
      "     |          Whether to predict raw scores.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration to predict.\n",
      "     |          If <= 0, starts from the first iteration.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Total number of iterations used in the prediction.\n",
      "     |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      "     |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |      pred_leaf : bool, optional (default=False)\n",
      "     |          Whether to predict leaf index.\n",
      "     |      pred_contrib : bool, optional (default=False)\n",
      "     |          Whether to predict feature contributions.\n",
      "     |      \n",
      "     |          .. note::\n",
      "     |      \n",
      "     |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      "     |              like SHAP interaction values,\n",
      "     |              you can install the shap package (https://github.com/slundberg/shap).\n",
      "     |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      "     |              column, where the last column is the expected value.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Other parameters for the prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predicted_result : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      "     |          The predicted values.\n",
      "     |      X_leaves : array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      "     |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      "     |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\n",
      "     |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      "     |  \n",
      "     |  predict_proba(self, X, raw_score=False, start_iteration=0, num_iteration=None, pred_leaf=False, pred_contrib=False, **kwargs)\n",
      "     |      Return the predicted probability for each class for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      raw_score : bool, optional (default=False)\n",
      "     |          Whether to predict raw scores.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration to predict.\n",
      "     |          If <= 0, starts from the first iteration.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Total number of iterations used in the prediction.\n",
      "     |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      "     |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |      pred_leaf : bool, optional (default=False)\n",
      "     |          Whether to predict leaf index.\n",
      "     |      pred_contrib : bool, optional (default=False)\n",
      "     |          Whether to predict feature contributions.\n",
      "     |      \n",
      "     |          .. note::\n",
      "     |      \n",
      "     |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      "     |              like SHAP interaction values,\n",
      "     |              you can install the shap package (https://github.com/slundberg/shap).\n",
      "     |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      "     |              column, where the last column is the expected value.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Other parameters for the prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predicted_probability : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      "     |          The predicted values.\n",
      "     |      X_leaves : array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      "     |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      "     |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\n",
      "     |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      :obj:`array` of shape = [n_classes]: The class label array.\n",
      "     |  \n",
      "     |  n_classes_\n",
      "     |      :obj:`int`: The number of classes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LGBMModel:\n",
      "     |  \n",
      "     |  __init__(self, boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs)\n",
      "     |      Construct a gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      boosting_type : string, optional (default='gbdt')\n",
      "     |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      "     |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      "     |          'goss', Gradient-based One-Side Sampling.\n",
      "     |          'rf', Random Forest.\n",
      "     |      num_leaves : int, optional (default=31)\n",
      "     |          Maximum tree leaves for base learners.\n",
      "     |      max_depth : int, optional (default=-1)\n",
      "     |          Maximum tree depth for base learners, <=0 means no limit.\n",
      "     |      learning_rate : float, optional (default=0.1)\n",
      "     |          Boosting learning rate.\n",
      "     |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      "     |          in training using ``reset_parameter`` callback.\n",
      "     |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      "     |      n_estimators : int, optional (default=100)\n",
      "     |          Number of boosted trees to fit.\n",
      "     |      subsample_for_bin : int, optional (default=200000)\n",
      "     |          Number of samples for constructing bins.\n",
      "     |      objective : string, callable or None, optional (default=None)\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      "     |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      "     |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |          Use this parameter only for multi-class classification task;\n",
      "     |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      "     |          Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      "     |          You may want to consider performing probability calibration\n",
      "     |          (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      "     |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      "     |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |          If None, all classes are supposed to have weight one.\n",
      "     |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      "     |          if ``sample_weight`` is specified.\n",
      "     |      min_split_gain : float, optional (default=0.)\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |      min_child_weight : float, optional (default=1e-3)\n",
      "     |          Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
      "     |      min_child_samples : int, optional (default=20)\n",
      "     |          Minimum number of data needed in a child (leaf).\n",
      "     |      subsample : float, optional (default=1.)\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      subsample_freq : int, optional (default=0)\n",
      "     |          Frequence of subsample, <=0 means no enable.\n",
      "     |      colsample_bytree : float, optional (default=1.)\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      reg_alpha : float, optional (default=0.)\n",
      "     |          L1 regularization term on weights.\n",
      "     |      reg_lambda : float, optional (default=0.)\n",
      "     |          L2 regularization term on weights.\n",
      "     |      random_state : int, RandomState object or None, optional (default=None)\n",
      "     |          Random number seed.\n",
      "     |          If int, this number is used to seed the C++ code.\n",
      "     |          If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      "     |          If None, default seeds in C++ code are used.\n",
      "     |      n_jobs : int, optional (default=-1)\n",
      "     |          Number of parallel threads.\n",
      "     |      silent : bool, optional (default=True)\n",
      "     |          Whether to print messages while running boosting.\n",
      "     |      importance_type : string, optional (default='split')\n",
      "     |          The type of feature importance to be filled into ``feature_importances_``.\n",
      "     |          If 'split', result contains numbers of times the feature is used in a model.\n",
      "     |          If 'gain', result contains total gains of splits which use the feature.\n",
      "     |      **kwargs\n",
      "     |          Other parameters for the model.\n",
      "     |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      "     |      \n",
      "     |          .. warning::\n",
      "     |      \n",
      "     |              \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      "     |      \n",
      "     |      Note\n",
      "     |      ----\n",
      "     |      A custom objective function can be provided for the ``objective`` parameter.\n",
      "     |      In this case, it should have the signature\n",
      "     |      ``objective(y_true, y_pred) -> grad, hess`` or\n",
      "     |      ``objective(y_true, y_pred, group) -> grad, hess``:\n",
      "     |      \n",
      "     |          y_true : array-like of shape = [n_samples]\n",
      "     |              The target values.\n",
      "     |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The predicted values.\n",
      "     |          group : array-like\n",
      "     |              Group/query data.\n",
      "     |              Only used in the learning-to-rank task.\n",
      "     |              sum(group) = n_samples.\n",
      "     |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |          grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The value of the first order derivative (gradient) for each sample point.\n",
      "     |          hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The value of the second order derivative (Hessian) for each sample point.\n",
      "     |      \n",
      "     |      For binary task, the y_pred is margin.\n",
      "     |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      "     |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
      "     |      and you should group grad and hess in this way as well.\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, optional (default=True)\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params\n",
      "     |          Parameter names with their new values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from LGBMModel:\n",
      "     |  \n",
      "     |  best_iteration_\n",
      "     |      :obj:`int` or :obj:`None`: The best iteration of fitted model if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  best_score_\n",
      "     |      :obj:`dict` or :obj:`None`: The best score of fitted model.\n",
      "     |  \n",
      "     |  booster_\n",
      "     |      Booster: The underlying Booster of this model.\n",
      "     |  \n",
      "     |  evals_result_\n",
      "     |      :obj:`dict` or :obj:`None`: The evaluation results if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      :obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |      \n",
      "     |          ``importance_type`` attribute is passed to the function\n",
      "     |          to configure the type of importance values to be extracted.\n",
      "     |  \n",
      "     |  feature_name_\n",
      "     |      :obj:`array` of shape = [n_features]: The names of features.\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  objective_\n",
      "     |      :obj:`string` or :obj:`callable`: The concrete objective used while fitting this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "    \n",
      "    class LGBMModel(sklearn.base.BaseEstimator)\n",
      "     |  LGBMModel(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the scikit-learn API for LightGBM.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LGBMModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs)\n",
      "     |      Construct a gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      boosting_type : string, optional (default='gbdt')\n",
      "     |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      "     |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      "     |          'goss', Gradient-based One-Side Sampling.\n",
      "     |          'rf', Random Forest.\n",
      "     |      num_leaves : int, optional (default=31)\n",
      "     |          Maximum tree leaves for base learners.\n",
      "     |      max_depth : int, optional (default=-1)\n",
      "     |          Maximum tree depth for base learners, <=0 means no limit.\n",
      "     |      learning_rate : float, optional (default=0.1)\n",
      "     |          Boosting learning rate.\n",
      "     |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      "     |          in training using ``reset_parameter`` callback.\n",
      "     |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      "     |      n_estimators : int, optional (default=100)\n",
      "     |          Number of boosted trees to fit.\n",
      "     |      subsample_for_bin : int, optional (default=200000)\n",
      "     |          Number of samples for constructing bins.\n",
      "     |      objective : string, callable or None, optional (default=None)\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      "     |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      "     |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |          Use this parameter only for multi-class classification task;\n",
      "     |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      "     |          Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      "     |          You may want to consider performing probability calibration\n",
      "     |          (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      "     |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      "     |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |          If None, all classes are supposed to have weight one.\n",
      "     |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      "     |          if ``sample_weight`` is specified.\n",
      "     |      min_split_gain : float, optional (default=0.)\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |      min_child_weight : float, optional (default=1e-3)\n",
      "     |          Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
      "     |      min_child_samples : int, optional (default=20)\n",
      "     |          Minimum number of data needed in a child (leaf).\n",
      "     |      subsample : float, optional (default=1.)\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      subsample_freq : int, optional (default=0)\n",
      "     |          Frequence of subsample, <=0 means no enable.\n",
      "     |      colsample_bytree : float, optional (default=1.)\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      reg_alpha : float, optional (default=0.)\n",
      "     |          L1 regularization term on weights.\n",
      "     |      reg_lambda : float, optional (default=0.)\n",
      "     |          L2 regularization term on weights.\n",
      "     |      random_state : int, RandomState object or None, optional (default=None)\n",
      "     |          Random number seed.\n",
      "     |          If int, this number is used to seed the C++ code.\n",
      "     |          If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      "     |          If None, default seeds in C++ code are used.\n",
      "     |      n_jobs : int, optional (default=-1)\n",
      "     |          Number of parallel threads.\n",
      "     |      silent : bool, optional (default=True)\n",
      "     |          Whether to print messages while running boosting.\n",
      "     |      importance_type : string, optional (default='split')\n",
      "     |          The type of feature importance to be filled into ``feature_importances_``.\n",
      "     |          If 'split', result contains numbers of times the feature is used in a model.\n",
      "     |          If 'gain', result contains total gains of splits which use the feature.\n",
      "     |      **kwargs\n",
      "     |          Other parameters for the model.\n",
      "     |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      "     |      \n",
      "     |          .. warning::\n",
      "     |      \n",
      "     |              \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      "     |      \n",
      "     |      Note\n",
      "     |      ----\n",
      "     |      A custom objective function can be provided for the ``objective`` parameter.\n",
      "     |      In this case, it should have the signature\n",
      "     |      ``objective(y_true, y_pred) -> grad, hess`` or\n",
      "     |      ``objective(y_true, y_pred, group) -> grad, hess``:\n",
      "     |      \n",
      "     |          y_true : array-like of shape = [n_samples]\n",
      "     |              The target values.\n",
      "     |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The predicted values.\n",
      "     |          group : array-like\n",
      "     |              Group/query data.\n",
      "     |              Only used in the learning-to-rank task.\n",
      "     |              sum(group) = n_samples.\n",
      "     |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |          grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The value of the first order derivative (gradient) for each sample point.\n",
      "     |          hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The value of the second order derivative (Hessian) for each sample point.\n",
      "     |      \n",
      "     |      For binary task, the y_pred is margin.\n",
      "     |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      "     |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
      "     |      and you should group grad and hess in this way as well.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, init_score=None, group=None, eval_set=None, eval_names=None, eval_sample_weight=None, eval_class_weight=None, eval_init_score=None, eval_group=None, eval_metric=None, early_stopping_rounds=None, verbose=True, feature_name='auto', categorical_feature='auto', callbacks=None, init_model=None)\n",
      "     |      Build a gradient boosting model from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          Input feature matrix.\n",
      "     |      y : array-like of shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in regression).\n",
      "     |      sample_weight : array-like of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Weights of training data.\n",
      "     |      init_score : array-like of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Init score of training data.\n",
      "     |      group : array-like or None, optional (default=None)\n",
      "     |          Group/query data.\n",
      "     |          Only used in the learning-to-rank task.\n",
      "     |          sum(group) = n_samples.\n",
      "     |          For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |          where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |      eval_set : list or None, optional (default=None)\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets.\n",
      "     |      eval_names : list of strings or None, optional (default=None)\n",
      "     |          Names of eval_set.\n",
      "     |      eval_sample_weight : list of arrays or None, optional (default=None)\n",
      "     |          Weights of eval data.\n",
      "     |      eval_class_weight : list or None, optional (default=None)\n",
      "     |          Class weights of eval data.\n",
      "     |      eval_init_score : list of arrays or None, optional (default=None)\n",
      "     |          Init score of eval data.\n",
      "     |      eval_group : list of arrays or None, optional (default=None)\n",
      "     |          Group data of eval data.\n",
      "     |      eval_metric : string, callable, list or None, optional (default=None)\n",
      "     |          If string, it should be a built-in evaluation metric to use.\n",
      "     |          If callable, it should be a custom evaluation metric, see note below for more details.\n",
      "     |          If list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both.\n",
      "     |          In either case, the ``metric`` from the model parameters will be evaluated and used as well.\n",
      "     |          Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker.\n",
      "     |      early_stopping_rounds : int or None, optional (default=None)\n",
      "     |          Activates early stopping. The model will train until the validation score stops improving.\n",
      "     |          Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
      "     |          to continue training.\n",
      "     |          Requires at least one validation data and one metric.\n",
      "     |          If there's more than one, will check all of them. But the training data is ignored anyway.\n",
      "     |          To check only the first metric, set the ``first_metric_only`` parameter to ``True``\n",
      "     |          in additional parameters ``**kwargs`` of the model constructor.\n",
      "     |      verbose : bool or int, optional (default=True)\n",
      "     |          Requires at least one evaluation data.\n",
      "     |          If True, the eval metric on the eval set is printed at each boosting stage.\n",
      "     |          If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n",
      "     |          The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      "     |      \n",
      "     |          .. rubric:: Example\n",
      "     |      \n",
      "     |          With ``verbose`` = 4 and at least one item in ``eval_set``,\n",
      "     |          an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      "     |      \n",
      "     |      feature_name : list of strings or 'auto', optional (default='auto')\n",
      "     |          Feature names.\n",
      "     |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "     |      categorical_feature : list of strings or int, or 'auto', optional (default='auto')\n",
      "     |          Categorical features.\n",
      "     |          If list of int, interpreted as indices.\n",
      "     |          If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "     |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "     |          All values in categorical features should be less than int32 max value (2147483647).\n",
      "     |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "     |          All negative values in categorical features will be treated as missing values.\n",
      "     |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "     |      callbacks : list of callback functions or None, optional (default=None)\n",
      "     |          List of callback functions that are applied at each iteration.\n",
      "     |          See Callbacks in Python API for more information.\n",
      "     |      init_model : string, Booster, LGBMModel or None, optional (default=None)\n",
      "     |          Filename of LightGBM model, Booster instance or LGBMModel instance used for continue training.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      Note\n",
      "     |      ----\n",
      "     |      Custom eval function expects a callable with following signatures:\n",
      "     |      ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n",
      "     |      ``func(y_true, y_pred, weight, group)``\n",
      "     |      and returns (eval_name, eval_result, is_higher_better) or\n",
      "     |      list of (eval_name, eval_result, is_higher_better):\n",
      "     |      \n",
      "     |          y_true : array-like of shape = [n_samples]\n",
      "     |              The target values.\n",
      "     |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The predicted values.\n",
      "     |          weight : array-like of shape = [n_samples]\n",
      "     |              The weight of samples.\n",
      "     |          group : array-like\n",
      "     |              Group/query data.\n",
      "     |              Only used in the learning-to-rank task.\n",
      "     |              sum(group) = n_samples.\n",
      "     |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |          eval_name : string\n",
      "     |              The name of evaluation function (without whitespaces).\n",
      "     |          eval_result : float\n",
      "     |              The eval result.\n",
      "     |          is_higher_better : bool\n",
      "     |              Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      "     |      \n",
      "     |      For binary task, the y_pred is probability of positive class (or margin in case of custom ``objective``).\n",
      "     |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      "     |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i].\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, optional (default=True)\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  predict(self, X, raw_score=False, start_iteration=0, num_iteration=None, pred_leaf=False, pred_contrib=False, **kwargs)\n",
      "     |      Return the predicted value for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      raw_score : bool, optional (default=False)\n",
      "     |          Whether to predict raw scores.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration to predict.\n",
      "     |          If <= 0, starts from the first iteration.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Total number of iterations used in the prediction.\n",
      "     |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      "     |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |      pred_leaf : bool, optional (default=False)\n",
      "     |          Whether to predict leaf index.\n",
      "     |      pred_contrib : bool, optional (default=False)\n",
      "     |          Whether to predict feature contributions.\n",
      "     |      \n",
      "     |          .. note::\n",
      "     |      \n",
      "     |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      "     |              like SHAP interaction values,\n",
      "     |              you can install the shap package (https://github.com/slundberg/shap).\n",
      "     |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      "     |              column, where the last column is the expected value.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Other parameters for the prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predicted_result : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      "     |          The predicted values.\n",
      "     |      X_leaves : array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      "     |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      "     |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\n",
      "     |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params\n",
      "     |          Parameter names with their new values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  best_iteration_\n",
      "     |      :obj:`int` or :obj:`None`: The best iteration of fitted model if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  best_score_\n",
      "     |      :obj:`dict` or :obj:`None`: The best score of fitted model.\n",
      "     |  \n",
      "     |  booster_\n",
      "     |      Booster: The underlying Booster of this model.\n",
      "     |  \n",
      "     |  evals_result_\n",
      "     |      :obj:`dict` or :obj:`None`: The evaluation results if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      :obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |      \n",
      "     |          ``importance_type`` attribute is passed to the function\n",
      "     |          to configure the type of importance values to be extracted.\n",
      "     |  \n",
      "     |  feature_name_\n",
      "     |      :obj:`array` of shape = [n_features]: The names of features.\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  objective_\n",
      "     |      :obj:`string` or :obj:`callable`: The concrete objective used while fitting this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LGBMRanker(LGBMModel)\n",
      "     |  LGBMRanker(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs)\n",
      "     |  \n",
      "     |  LightGBM ranker.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LGBMRanker\n",
      "     |      LGBMModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, init_score=None, group=None, eval_set=None, eval_names=None, eval_sample_weight=None, eval_init_score=None, eval_group=None, eval_metric=None, eval_at=(1, 2, 3, 4, 5), early_stopping_rounds=None, verbose=True, feature_name='auto', categorical_feature='auto', callbacks=None, init_model=None)\n",
      "     |      Build a gradient boosting model from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          Input feature matrix.\n",
      "     |      y : array-like of shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in regression).\n",
      "     |      sample_weight : array-like of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Weights of training data.\n",
      "     |      init_score : array-like of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Init score of training data.\n",
      "     |      group : array-like or None, optional (default=None)\n",
      "     |          Group/query data.\n",
      "     |          Only used in the learning-to-rank task.\n",
      "     |          sum(group) = n_samples.\n",
      "     |          For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |          where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |      eval_set : list or None, optional (default=None)\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets.\n",
      "     |      eval_names : list of strings or None, optional (default=None)\n",
      "     |          Names of eval_set.\n",
      "     |      eval_sample_weight : list of arrays or None, optional (default=None)\n",
      "     |          Weights of eval data.\n",
      "     |      eval_init_score : list of arrays or None, optional (default=None)\n",
      "     |          Init score of eval data.\n",
      "     |      eval_group : list of arrays or None, optional (default=None)\n",
      "     |          Group data of eval data.\n",
      "     |      eval_metric : string, callable, list or None, optional (default=None)\n",
      "     |          If string, it should be a built-in evaluation metric to use.\n",
      "     |          If callable, it should be a custom evaluation metric, see note below for more details.\n",
      "     |          If list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both.\n",
      "     |          In either case, the ``metric`` from the model parameters will be evaluated and used as well.\n",
      "     |          Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker.\n",
      "     |      eval_at : iterable of int, optional (default=(1, 2, 3, 4, 5))\n",
      "     |              The evaluation positions of the specified metric.\n",
      "     |          early_stopping_rounds : int or None, optional (default=None)\n",
      "     |          Activates early stopping. The model will train until the validation score stops improving.\n",
      "     |          Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
      "     |          to continue training.\n",
      "     |          Requires at least one validation data and one metric.\n",
      "     |          If there's more than one, will check all of them. But the training data is ignored anyway.\n",
      "     |          To check only the first metric, set the ``first_metric_only`` parameter to ``True``\n",
      "     |          in additional parameters ``**kwargs`` of the model constructor.\n",
      "     |      verbose : bool or int, optional (default=True)\n",
      "     |          Requires at least one evaluation data.\n",
      "     |          If True, the eval metric on the eval set is printed at each boosting stage.\n",
      "     |          If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n",
      "     |          The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      "     |      \n",
      "     |          .. rubric:: Example\n",
      "     |      \n",
      "     |          With ``verbose`` = 4 and at least one item in ``eval_set``,\n",
      "     |          an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      "     |      \n",
      "     |      feature_name : list of strings or 'auto', optional (default='auto')\n",
      "     |          Feature names.\n",
      "     |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "     |      categorical_feature : list of strings or int, or 'auto', optional (default='auto')\n",
      "     |          Categorical features.\n",
      "     |          If list of int, interpreted as indices.\n",
      "     |          If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "     |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "     |          All values in categorical features should be less than int32 max value (2147483647).\n",
      "     |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "     |          All negative values in categorical features will be treated as missing values.\n",
      "     |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "     |      callbacks : list of callback functions or None, optional (default=None)\n",
      "     |          List of callback functions that are applied at each iteration.\n",
      "     |          See Callbacks in Python API for more information.\n",
      "     |      init_model : string, Booster, LGBMModel or None, optional (default=None)\n",
      "     |          Filename of LightGBM model, Booster instance or LGBMModel instance used for continue training.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      Note\n",
      "     |      ----\n",
      "     |      Custom eval function expects a callable with following signatures:\n",
      "     |      ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n",
      "     |      ``func(y_true, y_pred, weight, group)``\n",
      "     |      and returns (eval_name, eval_result, is_higher_better) or\n",
      "     |      list of (eval_name, eval_result, is_higher_better):\n",
      "     |      \n",
      "     |          y_true : array-like of shape = [n_samples]\n",
      "     |              The target values.\n",
      "     |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The predicted values.\n",
      "     |          weight : array-like of shape = [n_samples]\n",
      "     |              The weight of samples.\n",
      "     |          group : array-like\n",
      "     |              Group/query data.\n",
      "     |              Only used in the learning-to-rank task.\n",
      "     |              sum(group) = n_samples.\n",
      "     |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |          eval_name : string\n",
      "     |              The name of evaluation function (without whitespaces).\n",
      "     |          eval_result : float\n",
      "     |              The eval result.\n",
      "     |          is_higher_better : bool\n",
      "     |              Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      "     |      \n",
      "     |      For binary task, the y_pred is probability of positive class (or margin in case of custom ``objective``).\n",
      "     |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      "     |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i].\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LGBMModel:\n",
      "     |  \n",
      "     |  __init__(self, boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs)\n",
      "     |      Construct a gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      boosting_type : string, optional (default='gbdt')\n",
      "     |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      "     |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      "     |          'goss', Gradient-based One-Side Sampling.\n",
      "     |          'rf', Random Forest.\n",
      "     |      num_leaves : int, optional (default=31)\n",
      "     |          Maximum tree leaves for base learners.\n",
      "     |      max_depth : int, optional (default=-1)\n",
      "     |          Maximum tree depth for base learners, <=0 means no limit.\n",
      "     |      learning_rate : float, optional (default=0.1)\n",
      "     |          Boosting learning rate.\n",
      "     |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      "     |          in training using ``reset_parameter`` callback.\n",
      "     |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      "     |      n_estimators : int, optional (default=100)\n",
      "     |          Number of boosted trees to fit.\n",
      "     |      subsample_for_bin : int, optional (default=200000)\n",
      "     |          Number of samples for constructing bins.\n",
      "     |      objective : string, callable or None, optional (default=None)\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      "     |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      "     |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |          Use this parameter only for multi-class classification task;\n",
      "     |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      "     |          Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      "     |          You may want to consider performing probability calibration\n",
      "     |          (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      "     |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      "     |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |          If None, all classes are supposed to have weight one.\n",
      "     |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      "     |          if ``sample_weight`` is specified.\n",
      "     |      min_split_gain : float, optional (default=0.)\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |      min_child_weight : float, optional (default=1e-3)\n",
      "     |          Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
      "     |      min_child_samples : int, optional (default=20)\n",
      "     |          Minimum number of data needed in a child (leaf).\n",
      "     |      subsample : float, optional (default=1.)\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      subsample_freq : int, optional (default=0)\n",
      "     |          Frequence of subsample, <=0 means no enable.\n",
      "     |      colsample_bytree : float, optional (default=1.)\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      reg_alpha : float, optional (default=0.)\n",
      "     |          L1 regularization term on weights.\n",
      "     |      reg_lambda : float, optional (default=0.)\n",
      "     |          L2 regularization term on weights.\n",
      "     |      random_state : int, RandomState object or None, optional (default=None)\n",
      "     |          Random number seed.\n",
      "     |          If int, this number is used to seed the C++ code.\n",
      "     |          If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      "     |          If None, default seeds in C++ code are used.\n",
      "     |      n_jobs : int, optional (default=-1)\n",
      "     |          Number of parallel threads.\n",
      "     |      silent : bool, optional (default=True)\n",
      "     |          Whether to print messages while running boosting.\n",
      "     |      importance_type : string, optional (default='split')\n",
      "     |          The type of feature importance to be filled into ``feature_importances_``.\n",
      "     |          If 'split', result contains numbers of times the feature is used in a model.\n",
      "     |          If 'gain', result contains total gains of splits which use the feature.\n",
      "     |      **kwargs\n",
      "     |          Other parameters for the model.\n",
      "     |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      "     |      \n",
      "     |          .. warning::\n",
      "     |      \n",
      "     |              \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      "     |      \n",
      "     |      Note\n",
      "     |      ----\n",
      "     |      A custom objective function can be provided for the ``objective`` parameter.\n",
      "     |      In this case, it should have the signature\n",
      "     |      ``objective(y_true, y_pred) -> grad, hess`` or\n",
      "     |      ``objective(y_true, y_pred, group) -> grad, hess``:\n",
      "     |      \n",
      "     |          y_true : array-like of shape = [n_samples]\n",
      "     |              The target values.\n",
      "     |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The predicted values.\n",
      "     |          group : array-like\n",
      "     |              Group/query data.\n",
      "     |              Only used in the learning-to-rank task.\n",
      "     |              sum(group) = n_samples.\n",
      "     |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |          grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The value of the first order derivative (gradient) for each sample point.\n",
      "     |          hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The value of the second order derivative (Hessian) for each sample point.\n",
      "     |      \n",
      "     |      For binary task, the y_pred is margin.\n",
      "     |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      "     |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
      "     |      and you should group grad and hess in this way as well.\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, optional (default=True)\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  predict(self, X, raw_score=False, start_iteration=0, num_iteration=None, pred_leaf=False, pred_contrib=False, **kwargs)\n",
      "     |      Return the predicted value for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      raw_score : bool, optional (default=False)\n",
      "     |          Whether to predict raw scores.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration to predict.\n",
      "     |          If <= 0, starts from the first iteration.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Total number of iterations used in the prediction.\n",
      "     |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      "     |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |      pred_leaf : bool, optional (default=False)\n",
      "     |          Whether to predict leaf index.\n",
      "     |      pred_contrib : bool, optional (default=False)\n",
      "     |          Whether to predict feature contributions.\n",
      "     |      \n",
      "     |          .. note::\n",
      "     |      \n",
      "     |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      "     |              like SHAP interaction values,\n",
      "     |              you can install the shap package (https://github.com/slundberg/shap).\n",
      "     |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      "     |              column, where the last column is the expected value.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Other parameters for the prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predicted_result : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      "     |          The predicted values.\n",
      "     |      X_leaves : array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      "     |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      "     |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\n",
      "     |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params\n",
      "     |          Parameter names with their new values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from LGBMModel:\n",
      "     |  \n",
      "     |  best_iteration_\n",
      "     |      :obj:`int` or :obj:`None`: The best iteration of fitted model if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  best_score_\n",
      "     |      :obj:`dict` or :obj:`None`: The best score of fitted model.\n",
      "     |  \n",
      "     |  booster_\n",
      "     |      Booster: The underlying Booster of this model.\n",
      "     |  \n",
      "     |  evals_result_\n",
      "     |      :obj:`dict` or :obj:`None`: The evaluation results if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      :obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |      \n",
      "     |          ``importance_type`` attribute is passed to the function\n",
      "     |          to configure the type of importance values to be extracted.\n",
      "     |  \n",
      "     |  feature_name_\n",
      "     |      :obj:`array` of shape = [n_features]: The names of features.\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  objective_\n",
      "     |      :obj:`string` or :obj:`callable`: The concrete objective used while fitting this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LGBMRegressor(LGBMModel, sklearn.base.RegressorMixin)\n",
      "     |  LGBMRegressor(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs)\n",
      "     |  \n",
      "     |  LightGBM regressor.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LGBMRegressor\n",
      "     |      LGBMModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, init_score=None, eval_set=None, eval_names=None, eval_sample_weight=None, eval_init_score=None, eval_metric=None, early_stopping_rounds=None, verbose=True, feature_name='auto', categorical_feature='auto', callbacks=None, init_model=None)\n",
      "     |      Build a gradient boosting model from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          Input feature matrix.\n",
      "     |      y : array-like of shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in regression).\n",
      "     |      sample_weight : array-like of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Weights of training data.\n",
      "     |      init_score : array-like of shape = [n_samples] or None, optional (default=None)\n",
      "     |          Init score of training data.\n",
      "     |      eval_set : list or None, optional (default=None)\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets.\n",
      "     |      eval_names : list of strings or None, optional (default=None)\n",
      "     |          Names of eval_set.\n",
      "     |      eval_sample_weight : list of arrays or None, optional (default=None)\n",
      "     |          Weights of eval data.\n",
      "     |      eval_init_score : list of arrays or None, optional (default=None)\n",
      "     |          Init score of eval data.\n",
      "     |      eval_metric : string, callable, list or None, optional (default=None)\n",
      "     |          If string, it should be a built-in evaluation metric to use.\n",
      "     |          If callable, it should be a custom evaluation metric, see note below for more details.\n",
      "     |          If list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both.\n",
      "     |          In either case, the ``metric`` from the model parameters will be evaluated and used as well.\n",
      "     |          Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker.\n",
      "     |      early_stopping_rounds : int or None, optional (default=None)\n",
      "     |          Activates early stopping. The model will train until the validation score stops improving.\n",
      "     |          Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
      "     |          to continue training.\n",
      "     |          Requires at least one validation data and one metric.\n",
      "     |          If there's more than one, will check all of them. But the training data is ignored anyway.\n",
      "     |          To check only the first metric, set the ``first_metric_only`` parameter to ``True``\n",
      "     |          in additional parameters ``**kwargs`` of the model constructor.\n",
      "     |      verbose : bool or int, optional (default=True)\n",
      "     |          Requires at least one evaluation data.\n",
      "     |          If True, the eval metric on the eval set is printed at each boosting stage.\n",
      "     |          If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n",
      "     |          The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      "     |      \n",
      "     |          .. rubric:: Example\n",
      "     |      \n",
      "     |          With ``verbose`` = 4 and at least one item in ``eval_set``,\n",
      "     |          an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      "     |      \n",
      "     |      feature_name : list of strings or 'auto', optional (default='auto')\n",
      "     |          Feature names.\n",
      "     |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "     |      categorical_feature : list of strings or int, or 'auto', optional (default='auto')\n",
      "     |          Categorical features.\n",
      "     |          If list of int, interpreted as indices.\n",
      "     |          If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "     |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "     |          All values in categorical features should be less than int32 max value (2147483647).\n",
      "     |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "     |          All negative values in categorical features will be treated as missing values.\n",
      "     |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "     |      callbacks : list of callback functions or None, optional (default=None)\n",
      "     |          List of callback functions that are applied at each iteration.\n",
      "     |          See Callbacks in Python API for more information.\n",
      "     |      init_model : string, Booster, LGBMModel or None, optional (default=None)\n",
      "     |          Filename of LightGBM model, Booster instance or LGBMModel instance used for continue training.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      Note\n",
      "     |      ----\n",
      "     |      Custom eval function expects a callable with following signatures:\n",
      "     |      ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n",
      "     |      ``func(y_true, y_pred, weight, group)``\n",
      "     |      and returns (eval_name, eval_result, is_higher_better) or\n",
      "     |      list of (eval_name, eval_result, is_higher_better):\n",
      "     |      \n",
      "     |          y_true : array-like of shape = [n_samples]\n",
      "     |              The target values.\n",
      "     |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The predicted values.\n",
      "     |          weight : array-like of shape = [n_samples]\n",
      "     |              The weight of samples.\n",
      "     |          group : array-like\n",
      "     |              Group/query data.\n",
      "     |              Only used in the learning-to-rank task.\n",
      "     |              sum(group) = n_samples.\n",
      "     |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |          eval_name : string\n",
      "     |              The name of evaluation function (without whitespaces).\n",
      "     |          eval_result : float\n",
      "     |              The eval result.\n",
      "     |          is_higher_better : bool\n",
      "     |              Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      "     |      \n",
      "     |      For binary task, the y_pred is probability of positive class (or margin in case of custom ``objective``).\n",
      "     |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      "     |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i].\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LGBMModel:\n",
      "     |  \n",
      "     |  __init__(self, boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs)\n",
      "     |      Construct a gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      boosting_type : string, optional (default='gbdt')\n",
      "     |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      "     |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      "     |          'goss', Gradient-based One-Side Sampling.\n",
      "     |          'rf', Random Forest.\n",
      "     |      num_leaves : int, optional (default=31)\n",
      "     |          Maximum tree leaves for base learners.\n",
      "     |      max_depth : int, optional (default=-1)\n",
      "     |          Maximum tree depth for base learners, <=0 means no limit.\n",
      "     |      learning_rate : float, optional (default=0.1)\n",
      "     |          Boosting learning rate.\n",
      "     |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      "     |          in training using ``reset_parameter`` callback.\n",
      "     |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      "     |      n_estimators : int, optional (default=100)\n",
      "     |          Number of boosted trees to fit.\n",
      "     |      subsample_for_bin : int, optional (default=200000)\n",
      "     |          Number of samples for constructing bins.\n",
      "     |      objective : string, callable or None, optional (default=None)\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      "     |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      "     |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |          Use this parameter only for multi-class classification task;\n",
      "     |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      "     |          Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      "     |          You may want to consider performing probability calibration\n",
      "     |          (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      "     |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      "     |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |          If None, all classes are supposed to have weight one.\n",
      "     |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      "     |          if ``sample_weight`` is specified.\n",
      "     |      min_split_gain : float, optional (default=0.)\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |      min_child_weight : float, optional (default=1e-3)\n",
      "     |          Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
      "     |      min_child_samples : int, optional (default=20)\n",
      "     |          Minimum number of data needed in a child (leaf).\n",
      "     |      subsample : float, optional (default=1.)\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      subsample_freq : int, optional (default=0)\n",
      "     |          Frequence of subsample, <=0 means no enable.\n",
      "     |      colsample_bytree : float, optional (default=1.)\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      reg_alpha : float, optional (default=0.)\n",
      "     |          L1 regularization term on weights.\n",
      "     |      reg_lambda : float, optional (default=0.)\n",
      "     |          L2 regularization term on weights.\n",
      "     |      random_state : int, RandomState object or None, optional (default=None)\n",
      "     |          Random number seed.\n",
      "     |          If int, this number is used to seed the C++ code.\n",
      "     |          If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      "     |          If None, default seeds in C++ code are used.\n",
      "     |      n_jobs : int, optional (default=-1)\n",
      "     |          Number of parallel threads.\n",
      "     |      silent : bool, optional (default=True)\n",
      "     |          Whether to print messages while running boosting.\n",
      "     |      importance_type : string, optional (default='split')\n",
      "     |          The type of feature importance to be filled into ``feature_importances_``.\n",
      "     |          If 'split', result contains numbers of times the feature is used in a model.\n",
      "     |          If 'gain', result contains total gains of splits which use the feature.\n",
      "     |      **kwargs\n",
      "     |          Other parameters for the model.\n",
      "     |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      "     |      \n",
      "     |          .. warning::\n",
      "     |      \n",
      "     |              \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      "     |      \n",
      "     |      Note\n",
      "     |      ----\n",
      "     |      A custom objective function can be provided for the ``objective`` parameter.\n",
      "     |      In this case, it should have the signature\n",
      "     |      ``objective(y_true, y_pred) -> grad, hess`` or\n",
      "     |      ``objective(y_true, y_pred, group) -> grad, hess``:\n",
      "     |      \n",
      "     |          y_true : array-like of shape = [n_samples]\n",
      "     |              The target values.\n",
      "     |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The predicted values.\n",
      "     |          group : array-like\n",
      "     |              Group/query data.\n",
      "     |              Only used in the learning-to-rank task.\n",
      "     |              sum(group) = n_samples.\n",
      "     |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "     |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "     |          grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The value of the first order derivative (gradient) for each sample point.\n",
      "     |          hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      "     |              The value of the second order derivative (Hessian) for each sample point.\n",
      "     |      \n",
      "     |      For binary task, the y_pred is margin.\n",
      "     |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      "     |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
      "     |      and you should group grad and hess in this way as well.\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, optional (default=True)\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  predict(self, X, raw_score=False, start_iteration=0, num_iteration=None, pred_leaf=False, pred_contrib=False, **kwargs)\n",
      "     |      Return the predicted value for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      raw_score : bool, optional (default=False)\n",
      "     |          Whether to predict raw scores.\n",
      "     |      start_iteration : int, optional (default=0)\n",
      "     |          Start index of the iteration to predict.\n",
      "     |          If <= 0, starts from the first iteration.\n",
      "     |      num_iteration : int or None, optional (default=None)\n",
      "     |          Total number of iterations used in the prediction.\n",
      "     |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      "     |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      "     |      pred_leaf : bool, optional (default=False)\n",
      "     |          Whether to predict leaf index.\n",
      "     |      pred_contrib : bool, optional (default=False)\n",
      "     |          Whether to predict feature contributions.\n",
      "     |      \n",
      "     |          .. note::\n",
      "     |      \n",
      "     |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      "     |              like SHAP interaction values,\n",
      "     |              you can install the shap package (https://github.com/slundberg/shap).\n",
      "     |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      "     |              column, where the last column is the expected value.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Other parameters for the prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predicted_result : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      "     |          The predicted values.\n",
      "     |      X_leaves : array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      "     |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      "     |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\n",
      "     |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params\n",
      "     |          Parameter names with their new values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from LGBMModel:\n",
      "     |  \n",
      "     |  best_iteration_\n",
      "     |      :obj:`int` or :obj:`None`: The best iteration of fitted model if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  best_score_\n",
      "     |      :obj:`dict` or :obj:`None`: The best score of fitted model.\n",
      "     |  \n",
      "     |  booster_\n",
      "     |      Booster: The underlying Booster of this model.\n",
      "     |  \n",
      "     |  evals_result_\n",
      "     |      :obj:`dict` or :obj:`None`: The evaluation results if ``early_stopping_rounds`` has been specified.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      :obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |      \n",
      "     |          ``importance_type`` attribute is passed to the function\n",
      "     |          to configure the type of importance values to be extracted.\n",
      "     |  \n",
      "     |  feature_name_\n",
      "     |      :obj:`array` of shape = [n_features]: The names of features.\n",
      "     |  \n",
      "     |  n_features_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      :obj:`int`: The number of features of fitted model.\n",
      "     |  \n",
      "     |  objective_\n",
      "     |      :obj:`string` or :obj:`callable`: The concrete objective used while fitting this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "\n",
      "FUNCTIONS\n",
      "    create_tree_digraph(booster, tree_index=0, show_info=None, precision=3, orientation='horizontal', **kwargs)\n",
      "        Create a digraph representation of specified tree.\n",
      "        \n",
      "        Each node in the graph represents a node in the tree.\n",
      "        \n",
      "        Non-leaf nodes have labels like ``Column_10 <= 875.9``, which means\n",
      "        \"this node splits on the feature named \"Column_10\", with threshold 875.9\".\n",
      "        \n",
      "        Leaf nodes have labels like ``leaf 2: 0.422``, which means \"this node is a\n",
      "        leaf node, and the predicted value for records that fall into this node\n",
      "        is 0.422\". The number (``2``) is an internal unique identifier and doesn't\n",
      "        have any special meaning.\n",
      "        \n",
      "        .. note::\n",
      "        \n",
      "            For more information please visit\n",
      "            https://graphviz.readthedocs.io/en/stable/api.html#digraph.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster or LGBMModel\n",
      "            Booster or LGBMModel instance to be converted.\n",
      "        tree_index : int, optional (default=0)\n",
      "            The index of a target tree to convert.\n",
      "        show_info : list of strings or None, optional (default=None)\n",
      "            What information should be shown in nodes.\n",
      "        \n",
      "                - ``'split_gain'`` : gain from adding this split to the model\n",
      "                - ``'internal_value'`` : raw predicted value that would be produced by this node if it was a leaf node\n",
      "                - ``'internal_count'`` : number of records from the training data that fall into this non-leaf node\n",
      "                - ``'internal_weight'`` : total weight of all nodes that fall into this non-leaf node\n",
      "                - ``'leaf_count'`` : number of records from the training data that fall into this leaf node\n",
      "                - ``'leaf_weight'`` : total weight (sum of hessian) of all observations that fall into this leaf node\n",
      "                - ``'data_percentage'`` : percentage of training data that fall into this node\n",
      "        precision : int or None, optional (default=3)\n",
      "            Used to restrict the display of floating point values to a certain precision.\n",
      "        orientation : string, optional (default='horizontal')\n",
      "            Orientation of the tree.\n",
      "            Can be 'horizontal' or 'vertical'.\n",
      "        **kwargs\n",
      "            Other parameters passed to ``Digraph`` constructor.\n",
      "            Check https://graphviz.readthedocs.io/en/stable/api.html#digraph for the full list of supported parameters.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        graph : graphviz.Digraph\n",
      "            The digraph representation of specified tree.\n",
      "    \n",
      "    cv(params, train_set, num_boost_round=100, folds=None, nfold=5, stratified=True, shuffle=True, metrics=None, fobj=None, feval=None, init_model=None, feature_name='auto', categorical_feature='auto', early_stopping_rounds=None, fpreproc=None, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, eval_train_metric=False, return_cvbooster=False)\n",
      "        Perform the cross-validation with given paramaters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Parameters for Booster.\n",
      "        train_set : Dataset\n",
      "            Data to be trained on.\n",
      "        num_boost_round : int, optional (default=100)\n",
      "            Number of boosting iterations.\n",
      "        folds : generator or iterator of (train_idx, test_idx) tuples, scikit-learn splitter object or None, optional (default=None)\n",
      "            If generator or iterator, it should yield the train and test indices for each fold.\n",
      "            If object, it should be one of the scikit-learn splitter classes\n",
      "            (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      "            and have ``split`` method.\n",
      "            This argument has highest priority over other data split arguments.\n",
      "        nfold : int, optional (default=5)\n",
      "            Number of folds in CV.\n",
      "        stratified : bool, optional (default=True)\n",
      "            Whether to perform stratified sampling.\n",
      "        shuffle : bool, optional (default=True)\n",
      "            Whether to shuffle before splitting data.\n",
      "        metrics : string, list of strings or None, optional (default=None)\n",
      "            Evaluation metrics to be monitored while CV.\n",
      "            If not None, the metric in ``params`` will be overridden.\n",
      "        fobj : callable or None, optional (default=None)\n",
      "            Customized objective function.\n",
      "            Should accept two parameters: preds, train_data,\n",
      "            and return (grad, hess).\n",
      "        \n",
      "                preds : list or numpy 1-D array\n",
      "                    The predicted values.\n",
      "                train_data : Dataset\n",
      "                    The training dataset.\n",
      "                grad : list or numpy 1-D array\n",
      "                    The value of the first order derivative (gradient) for each sample point.\n",
      "                hess : list or numpy 1-D array\n",
      "                    The value of the second order derivative (Hessian) for each sample point.\n",
      "        \n",
      "            For binary task, the preds is margin.\n",
      "            For multi-class task, the preds is group by class_id first, then group by row_id.\n",
      "            If you want to get i-th row preds in j-th class, the access way is score[j * num_data + i]\n",
      "            and you should group grad and hess in this way as well.\n",
      "        \n",
      "        feval : callable, list of callable functions or None, optional (default=None)\n",
      "            Customized evaluation function.\n",
      "            Each evaluation function should accept two parameters: preds, train_data,\n",
      "            and return (eval_name, eval_result, is_higher_better) or list of such tuples.\n",
      "        \n",
      "                preds : list or numpy 1-D array\n",
      "                    The predicted values.\n",
      "                train_data : Dataset\n",
      "                    The training dataset.\n",
      "                eval_name : string\n",
      "                    The name of evaluation function (without whitespaces).\n",
      "                eval_result : float\n",
      "                    The eval result.\n",
      "                is_higher_better : bool\n",
      "                    Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      "        \n",
      "            For binary task, the preds is probability of positive class (or margin in case of specified ``fobj``).\n",
      "            For multi-class task, the preds is group by class_id first, then group by row_id.\n",
      "            If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i].\n",
      "            To ignore the default metric corresponding to the used objective,\n",
      "            set ``metrics`` to the string ``\"None\"``.\n",
      "        init_model : string, Booster or None, optional (default=None)\n",
      "            Filename of LightGBM model or Booster instance used for continue training.\n",
      "        feature_name : list of strings or 'auto', optional (default=\"auto\")\n",
      "            Feature names.\n",
      "            If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "        categorical_feature : list of strings or int, or 'auto', optional (default=\"auto\")\n",
      "            Categorical features.\n",
      "            If list of int, interpreted as indices.\n",
      "            If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "            If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "            All values in categorical features should be less than int32 max value (2147483647).\n",
      "            Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "            All negative values in categorical features will be treated as missing values.\n",
      "            The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "        early_stopping_rounds : int or None, optional (default=None)\n",
      "            Activates early stopping.\n",
      "            CV score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
      "            to continue.\n",
      "            Requires at least one metric. If there's more than one, will check all of them.\n",
      "            To check only the first metric, set the ``first_metric_only`` parameter to ``True`` in ``params``.\n",
      "            Last entry in evaluation history is the one from the best iteration.\n",
      "        fpreproc : callable or None, optional (default=None)\n",
      "            Preprocessing function that takes (dtrain, dtest, params)\n",
      "            and returns transformed versions of those.\n",
      "        verbose_eval : bool, int, or None, optional (default=None)\n",
      "            Whether to display the progress.\n",
      "            If None, progress will be displayed when np.ndarray is returned.\n",
      "            If True, progress will be displayed at every boosting stage.\n",
      "            If int, progress will be displayed at every given ``verbose_eval`` boosting stage.\n",
      "        show_stdv : bool, optional (default=True)\n",
      "            Whether to display the standard deviation in progress.\n",
      "            Results are not affected by this parameter, and always contain std.\n",
      "        seed : int, optional (default=0)\n",
      "            Seed used to generate the folds (passed to numpy.random.seed).\n",
      "        callbacks : list of callables or None, optional (default=None)\n",
      "            List of callback functions that are applied at each iteration.\n",
      "            See Callbacks in Python API for more information.\n",
      "        eval_train_metric : bool, optional (default=False)\n",
      "            Whether to display the train metric in progress.\n",
      "            The score of the metric is calculated again after each training step, so there is some impact on performance.\n",
      "        return_cvbooster : bool, optional (default=False)\n",
      "            Whether to return Booster models trained on each fold through ``CVBooster``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        eval_hist : dict\n",
      "            Evaluation history.\n",
      "            The dictionary has the following format:\n",
      "            {'metric1-mean': [values], 'metric1-stdv': [values],\n",
      "            'metric2-mean': [values], 'metric2-stdv': [values],\n",
      "            ...}.\n",
      "            If ``return_cvbooster=True``, also returns trained boosters via ``cvbooster`` key.\n",
      "    \n",
      "    early_stopping(stopping_rounds: int, first_metric_only: bool = False, verbose: bool = True) -> Callable\n",
      "        Create a callback that activates early stopping.\n",
      "        \n",
      "        Activates early stopping.\n",
      "        The model will train until the validation score stops improving.\n",
      "        Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
      "        to continue training.\n",
      "        Requires at least one validation data and one metric.\n",
      "        If there's more than one, will check all of them. But the training data is ignored anyway.\n",
      "        To check only the first metric set ``first_metric_only`` to True.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        stopping_rounds : int\n",
      "           The possible number of rounds without the trend occurrence.\n",
      "        first_metric_only : bool, optional (default=False)\n",
      "           Whether to use only the first metric for early stopping.\n",
      "        verbose : bool, optional (default=True)\n",
      "            Whether to print message with early stopping information.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        callback : function\n",
      "            The callback that activates early stopping.\n",
      "    \n",
      "    plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance', xlabel='Feature importance', ylabel='Features', importance_type='split', max_num_features=None, ignore_zero=True, figsize=None, dpi=None, grid=True, precision=3, **kwargs)\n",
      "        Plot model's feature importances.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster or LGBMModel\n",
      "            Booster or LGBMModel instance which feature importance should be plotted.\n",
      "        ax : matplotlib.axes.Axes or None, optional (default=None)\n",
      "            Target axes instance.\n",
      "            If None, new figure and axes will be created.\n",
      "        height : float, optional (default=0.2)\n",
      "            Bar height, passed to ``ax.barh()``.\n",
      "        xlim : tuple of 2 elements or None, optional (default=None)\n",
      "            Tuple passed to ``ax.xlim()``.\n",
      "        ylim : tuple of 2 elements or None, optional (default=None)\n",
      "            Tuple passed to ``ax.ylim()``.\n",
      "        title : string or None, optional (default=\"Feature importance\")\n",
      "            Axes title.\n",
      "            If None, title is disabled.\n",
      "        xlabel : string or None, optional (default=\"Feature importance\")\n",
      "            X-axis title label.\n",
      "            If None, title is disabled.\n",
      "        ylabel : string or None, optional (default=\"Features\")\n",
      "            Y-axis title label.\n",
      "            If None, title is disabled.\n",
      "        importance_type : string, optional (default=\"split\")\n",
      "            How the importance is calculated.\n",
      "            If \"split\", result contains numbers of times the feature is used in a model.\n",
      "            If \"gain\", result contains total gains of splits which use the feature.\n",
      "        max_num_features : int or None, optional (default=None)\n",
      "            Max number of top features displayed on plot.\n",
      "            If None or <1, all features will be displayed.\n",
      "        ignore_zero : bool, optional (default=True)\n",
      "            Whether to ignore features with zero importance.\n",
      "        figsize : tuple of 2 elements or None, optional (default=None)\n",
      "            Figure size.\n",
      "        dpi : int or None, optional (default=None)\n",
      "            Resolution of the figure.\n",
      "        grid : bool, optional (default=True)\n",
      "            Whether to add a grid for axes.\n",
      "        precision : int or None, optional (default=3)\n",
      "            Used to restrict the display of floating point values to a certain precision.\n",
      "        **kwargs\n",
      "            Other parameters passed to ``ax.barh()``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib.axes.Axes\n",
      "            The plot with model's feature importances.\n",
      "    \n",
      "    plot_metric(booster, metric=None, dataset_names=None, ax=None, xlim=None, ylim=None, title='Metric during training', xlabel='Iterations', ylabel='auto', figsize=None, dpi=None, grid=True)\n",
      "        Plot one metric during training.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : dict or LGBMModel\n",
      "            Dictionary returned from ``lightgbm.train()`` or LGBMModel instance.\n",
      "        metric : string or None, optional (default=None)\n",
      "            The metric name to plot.\n",
      "            Only one metric supported because different metrics have various scales.\n",
      "            If None, first metric picked from dictionary (according to hashcode).\n",
      "        dataset_names : list of strings or None, optional (default=None)\n",
      "            List of the dataset names which are used to calculate metric to plot.\n",
      "            If None, all datasets are used.\n",
      "        ax : matplotlib.axes.Axes or None, optional (default=None)\n",
      "            Target axes instance.\n",
      "            If None, new figure and axes will be created.\n",
      "        xlim : tuple of 2 elements or None, optional (default=None)\n",
      "            Tuple passed to ``ax.xlim()``.\n",
      "        ylim : tuple of 2 elements or None, optional (default=None)\n",
      "            Tuple passed to ``ax.ylim()``.\n",
      "        title : string or None, optional (default=\"Metric during training\")\n",
      "            Axes title.\n",
      "            If None, title is disabled.\n",
      "        xlabel : string or None, optional (default=\"Iterations\")\n",
      "            X-axis title label.\n",
      "            If None, title is disabled.\n",
      "        ylabel : string or None, optional (default=\"auto\")\n",
      "            Y-axis title label.\n",
      "            If 'auto', metric name is used.\n",
      "            If None, title is disabled.\n",
      "        figsize : tuple of 2 elements or None, optional (default=None)\n",
      "            Figure size.\n",
      "        dpi : int or None, optional (default=None)\n",
      "            Resolution of the figure.\n",
      "        grid : bool, optional (default=True)\n",
      "            Whether to add a grid for axes.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib.axes.Axes\n",
      "            The plot with metric's history over the training.\n",
      "    \n",
      "    plot_split_value_histogram(booster, feature, bins=None, ax=None, width_coef=0.8, xlim=None, ylim=None, title='Split value histogram for feature with @index/name@ @feature@', xlabel='Feature split value', ylabel='Count', figsize=None, dpi=None, grid=True, **kwargs)\n",
      "        Plot split value histogram for the specified feature of the model.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster or LGBMModel\n",
      "            Booster or LGBMModel instance of which feature split value histogram should be plotted.\n",
      "        feature : int or string\n",
      "            The feature name or index the histogram is plotted for.\n",
      "            If int, interpreted as index.\n",
      "            If string, interpreted as name.\n",
      "        bins : int, string or None, optional (default=None)\n",
      "            The maximum number of bins.\n",
      "            If None, the number of bins equals number of unique split values.\n",
      "            If string, it should be one from the list of the supported values by ``numpy.histogram()`` function.\n",
      "        ax : matplotlib.axes.Axes or None, optional (default=None)\n",
      "            Target axes instance.\n",
      "            If None, new figure and axes will be created.\n",
      "        width_coef : float, optional (default=0.8)\n",
      "            Coefficient for histogram bar width.\n",
      "        xlim : tuple of 2 elements or None, optional (default=None)\n",
      "            Tuple passed to ``ax.xlim()``.\n",
      "        ylim : tuple of 2 elements or None, optional (default=None)\n",
      "            Tuple passed to ``ax.ylim()``.\n",
      "        title : string or None, optional (default=\"Split value histogram for feature with @index/name@ @feature@\")\n",
      "            Axes title.\n",
      "            If None, title is disabled.\n",
      "            @feature@ placeholder can be used, and it will be replaced with the value of ``feature`` parameter.\n",
      "            @index/name@ placeholder can be used,\n",
      "            and it will be replaced with ``index`` word in case of ``int`` type ``feature`` parameter\n",
      "            or ``name`` word in case of ``string`` type ``feature`` parameter.\n",
      "        xlabel : string or None, optional (default=\"Feature split value\")\n",
      "            X-axis title label.\n",
      "            If None, title is disabled.\n",
      "        ylabel : string or None, optional (default=\"Count\")\n",
      "            Y-axis title label.\n",
      "            If None, title is disabled.\n",
      "        figsize : tuple of 2 elements or None, optional (default=None)\n",
      "            Figure size.\n",
      "        dpi : int or None, optional (default=None)\n",
      "            Resolution of the figure.\n",
      "        grid : bool, optional (default=True)\n",
      "            Whether to add a grid for axes.\n",
      "        **kwargs\n",
      "            Other parameters passed to ``ax.bar()``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib.axes.Axes\n",
      "            The plot with specified model's feature split value histogram.\n",
      "    \n",
      "    plot_tree(booster, ax=None, tree_index=0, figsize=None, dpi=None, show_info=None, precision=3, orientation='horizontal', **kwargs)\n",
      "        Plot specified tree.\n",
      "        \n",
      "        Each node in the graph represents a node in the tree.\n",
      "        \n",
      "        Non-leaf nodes have labels like ``Column_10 <= 875.9``, which means\n",
      "        \"this node splits on the feature named \"Column_10\", with threshold 875.9\".\n",
      "        \n",
      "        Leaf nodes have labels like ``leaf 2: 0.422``, which means \"this node is a\n",
      "        leaf node, and the predicted value for records that fall into this node\n",
      "        is 0.422\". The number (``2``) is an internal unique identifier and doesn't\n",
      "        have any special meaning.\n",
      "        \n",
      "        .. note::\n",
      "        \n",
      "            It is preferable to use ``create_tree_digraph()`` because of its lossless quality\n",
      "            and returned objects can be also rendered and displayed directly inside a Jupyter notebook.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster or LGBMModel\n",
      "            Booster or LGBMModel instance to be plotted.\n",
      "        ax : matplotlib.axes.Axes or None, optional (default=None)\n",
      "            Target axes instance.\n",
      "            If None, new figure and axes will be created.\n",
      "        tree_index : int, optional (default=0)\n",
      "            The index of a target tree to plot.\n",
      "        figsize : tuple of 2 elements or None, optional (default=None)\n",
      "            Figure size.\n",
      "        dpi : int or None, optional (default=None)\n",
      "            Resolution of the figure.\n",
      "        show_info : list of strings or None, optional (default=None)\n",
      "            What information should be shown in nodes.\n",
      "        \n",
      "                - ``'split_gain'`` : gain from adding this split to the model\n",
      "                - ``'internal_value'`` : raw predicted value that would be produced by this node if it was a leaf node\n",
      "                - ``'internal_count'`` : number of records from the training data that fall into this non-leaf node\n",
      "                - ``'internal_weight'`` : total weight of all nodes that fall into this non-leaf node\n",
      "                - ``'leaf_count'`` : number of records from the training data that fall into this leaf node\n",
      "                - ``'leaf_weight'`` : total weight (sum of hessian) of all observations that fall into this leaf node\n",
      "                - ``'data_percentage'`` : percentage of training data that fall into this node\n",
      "        precision : int or None, optional (default=3)\n",
      "            Used to restrict the display of floating point values to a certain precision.\n",
      "        orientation : string, optional (default='horizontal')\n",
      "            Orientation of the tree.\n",
      "            Can be 'horizontal' or 'vertical'.\n",
      "        **kwargs\n",
      "            Other parameters passed to ``Digraph`` constructor.\n",
      "            Check https://graphviz.readthedocs.io/en/stable/api.html#digraph for the full list of supported parameters.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib.axes.Axes\n",
      "            The plot with single tree.\n",
      "    \n",
      "    print_evaluation(period: int = 1, show_stdv: bool = True) -> Callable\n",
      "        Create a callback that prints the evaluation results.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        period : int, optional (default=1)\n",
      "            The period to print the evaluation results.\n",
      "        show_stdv : bool, optional (default=True)\n",
      "            Whether to show stdv (if provided).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        callback : function\n",
      "            The callback that prints the evaluation results every ``period`` iteration(s).\n",
      "    \n",
      "    record_evaluation(eval_result: Dict[str, Dict[str, List[Any]]]) -> Callable\n",
      "        Create a callback that records the evaluation history into ``eval_result``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        eval_result : dict\n",
      "           A dictionary to store the evaluation results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        callback : function\n",
      "            The callback that records the evaluation history into the passed dictionary.\n",
      "    \n",
      "    register_logger(logger)\n",
      "        Register custom logger.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        logger : logging.Logger\n",
      "            Custom logger.\n",
      "    \n",
      "    reset_parameter(**kwargs: Union[list, Callable]) -> Callable\n",
      "        Create a callback that resets the parameter after the first iteration.\n",
      "        \n",
      "        .. note::\n",
      "        \n",
      "            The initial parameter will still take in-effect on first iteration.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        **kwargs : value should be list or function\n",
      "            List of parameters for each boosting round\n",
      "            or a customized function that calculates the parameter in terms of\n",
      "            current number of round (e.g. yields learning rate decay).\n",
      "            If list lst, parameter = lst[current_round].\n",
      "            If function func, parameter = func(current_round).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        callback : function\n",
      "            The callback that resets the parameter after the first iteration.\n",
      "    \n",
      "    train(params, train_set, num_boost_round=100, valid_sets=None, valid_names=None, fobj=None, feval=None, init_model=None, feature_name='auto', categorical_feature='auto', early_stopping_rounds=None, evals_result=None, verbose_eval=True, learning_rates=None, keep_training_booster=False, callbacks=None)\n",
      "        Perform the training with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Parameters for training.\n",
      "        train_set : Dataset\n",
      "            Data to be trained on.\n",
      "        num_boost_round : int, optional (default=100)\n",
      "            Number of boosting iterations.\n",
      "        valid_sets : list of Datasets or None, optional (default=None)\n",
      "            List of data to be evaluated on during training.\n",
      "        valid_names : list of strings or None, optional (default=None)\n",
      "            Names of ``valid_sets``.\n",
      "        fobj : callable or None, optional (default=None)\n",
      "            Customized objective function.\n",
      "            Should accept two parameters: preds, train_data,\n",
      "            and return (grad, hess).\n",
      "        \n",
      "                preds : list or numpy 1-D array\n",
      "                    The predicted values.\n",
      "                train_data : Dataset\n",
      "                    The training dataset.\n",
      "                grad : list or numpy 1-D array\n",
      "                    The value of the first order derivative (gradient) for each sample point.\n",
      "                hess : list or numpy 1-D array\n",
      "                    The value of the second order derivative (Hessian) for each sample point.\n",
      "        \n",
      "            For binary task, the preds is margin.\n",
      "            For multi-class task, the preds is group by class_id first, then group by row_id.\n",
      "            If you want to get i-th row preds in j-th class, the access way is score[j * num_data + i]\n",
      "            and you should group grad and hess in this way as well.\n",
      "        \n",
      "        feval : callable, list of callable functions or None, optional (default=None)\n",
      "            Customized evaluation function.\n",
      "            Each evaluation function should accept two parameters: preds, train_data,\n",
      "            and return (eval_name, eval_result, is_higher_better) or list of such tuples.\n",
      "        \n",
      "                preds : list or numpy 1-D array\n",
      "                    The predicted values.\n",
      "                train_data : Dataset\n",
      "                    The training dataset.\n",
      "                eval_name : string\n",
      "                    The name of evaluation function (without whitespaces).\n",
      "                eval_result : float\n",
      "                    The eval result.\n",
      "                is_higher_better : bool\n",
      "                    Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      "        \n",
      "            For binary task, the preds is probability of positive class (or margin in case of specified ``fobj``).\n",
      "            For multi-class task, the preds is group by class_id first, then group by row_id.\n",
      "            If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i].\n",
      "            To ignore the default metric corresponding to the used objective,\n",
      "            set the ``metric`` parameter to the string ``\"None\"`` in ``params``.\n",
      "        init_model : string, Booster or None, optional (default=None)\n",
      "            Filename of LightGBM model or Booster instance used for continue training.\n",
      "        feature_name : list of strings or 'auto', optional (default=\"auto\")\n",
      "            Feature names.\n",
      "            If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "        categorical_feature : list of strings or int, or 'auto', optional (default=\"auto\")\n",
      "            Categorical features.\n",
      "            If list of int, interpreted as indices.\n",
      "            If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "            If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "            All values in categorical features should be less than int32 max value (2147483647).\n",
      "            Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "            All negative values in categorical features will be treated as missing values.\n",
      "            The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "        early_stopping_rounds : int or None, optional (default=None)\n",
      "            Activates early stopping. The model will train until the validation score stops improving.\n",
      "            Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
      "            to continue training.\n",
      "            Requires at least one validation data and one metric.\n",
      "            If there's more than one, will check all of them. But the training data is ignored anyway.\n",
      "            To check only the first metric, set the ``first_metric_only`` parameter to ``True`` in ``params``.\n",
      "            The index of iteration that has the best performance will be saved in the ``best_iteration`` field\n",
      "            if early stopping logic is enabled by setting ``early_stopping_rounds``.\n",
      "        evals_result: dict or None, optional (default=None)\n",
      "            This dictionary used to store all evaluation results of all the items in ``valid_sets``.\n",
      "        \n",
      "            .. rubric:: Example\n",
      "        \n",
      "            With a ``valid_sets`` = [valid_set, train_set],\n",
      "            ``valid_names`` = ['eval', 'train']\n",
      "            and a ``params`` = {'metric': 'logloss'}\n",
      "            returns {'train': {'logloss': ['0.48253', '0.35953', ...]},\n",
      "            'eval': {'logloss': ['0.480385', '0.357756', ...]}}.\n",
      "        \n",
      "        verbose_eval : bool or int, optional (default=True)\n",
      "            Requires at least one validation data.\n",
      "            If True, the eval metric on the valid set is printed at each boosting stage.\n",
      "            If int, the eval metric on the valid set is printed at every ``verbose_eval`` boosting stage.\n",
      "            The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      "        \n",
      "            .. rubric:: Example\n",
      "        \n",
      "            With ``verbose_eval`` = 4 and at least one item in ``valid_sets``,\n",
      "            an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      "        \n",
      "        learning_rates : list, callable or None, optional (default=None)\n",
      "            List of learning rates for each boosting round\n",
      "            or a customized function that calculates ``learning_rate``\n",
      "            in terms of current number of round (e.g. yields learning rate decay).\n",
      "        keep_training_booster : bool, optional (default=False)\n",
      "            Whether the returned Booster will be used to keep training.\n",
      "            If False, the returned value will be converted into _InnerPredictor before returning.\n",
      "            When your model is very large and cause the memory error,\n",
      "            you can try to set this param to ``True`` to avoid the model conversion performed during the internal call of ``model_to_string``.\n",
      "            You can still use _InnerPredictor as ``init_model`` for future continue training.\n",
      "        callbacks : list of callables or None, optional (default=None)\n",
      "            List of callback functions that are applied at each iteration.\n",
      "            See Callbacks in Python API for more information.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        booster : Booster\n",
      "            The trained Booster model.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Dataset', 'Booster', 'CVBooster', 'register_logger', 'trai...\n",
      "\n",
      "VERSION\n",
      "    3.2.1\n",
      "\n",
      "FILE\n",
      "    c:\\users\\yeeunlee\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-ridge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
